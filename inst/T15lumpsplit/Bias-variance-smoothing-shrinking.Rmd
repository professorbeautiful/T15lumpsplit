---
title: "Bias, variance, smoothing, and shrinking"
resource_files:
- www/zoomAdvice.Rmd
- www/zoom_triggers.js
- www/readInnerWindow.js
- www/the-delta-method.pdf
- www/BayesEquation.Rmd
runtime: shiny
output:
  html_document:
    number_sections: yes
    toc: yes
    toc_depth: 6
    toc_float:
      collapsed: no
      smooth_scroll: no
---
```{r setup, echo = FALSE}
knitr::opts_chunk$set(echo = FALSE)
options(knitr.duplicate.label = 'allow')
shiny::addResourcePath("shinyjs", system.file("srcjs", package = "shinyjs"))
```

```{r, context="server", message=FALSE, warning=FALSE}
## ok, adding message=FALSE, warning=FALSE succeeds in suppressing
## the output at shinyapps.
## Furthermore, all the other problems on Chrome at shinyapps
## have disappeared!
invisible(require(shinyjs) )
shinyjs::useShinyjs(rmd = TRUE)
```

<script src="www/readCookie.js"></script>
<script src="shinyjs/inject.js"></script>


\[
\newcommand{\mylarge}[1] {\large{#1}\normalsize}
\newcommand{\mynormal}[1] {{\normalsize{#1}}}
\]

```{r, include=FALSE}
cookieID = 'cookieID'
shinycookie::initShinyCookie(cookieID, timeout = 500)
# timeout is The length of time (in milliseconds) to poll for changes to cookies
# I guess you could set up a listener on cookieID.
#shinycookie::updateCookie(
#  shiny::getDefaultReactiveDomain(), TQ2='I am TQ2')
# This works. Not printing NULL
```


```{r shinyjs, echo=FALSE, message=FALSE, warning=FALSE}

#shiny::addResourcePath("shinyjs", system.file("srcjs", package = "shinyjs"))
#library(shinyjs)
```

```{r readCookie}
shinyjs::extendShinyjs(script = 'www/readCookie.js', functions=c('readCookie'))

#  This works in JS:  shinyjs.click(document.getElementById('downloadAllUserEntries') )

saveEntriesJScode =  
  "shinyjs.saveEntriesJS = function() {document.getElementById('downloadAllUserEntries').click();}"
helloJScode = 
  "shinyjs.helloJS = function() {alert('HELLO');};"
shinyjs::extendShinyjs(text = saveEntriesJScode, functions=c('saveEntriesJS'))
shinyjs::extendShinyjs(text = helloJScode, functions=c('helloJS'))

jsCode <- "shinyjs.pageCol = function(params){$('body').css('background', params);}"
extendShinyjs(text = jsCode, functions = c("pageCol"))

```


```{r addCustomMessageHandler, eval=FALSE}
# This was printing NULL
#conflict with extendShinyjs?
tags$head(tags$script(
    HTML('
      Shiny.addCustomMessageHandler ("readCookie",function (message) {
        var mycookie = readCookie(message.name);
        Shiny.onInputChange("cookie", mycookie);
      })

      function readCookie(name) {
        var nameEQ = name + "=";
        var ca = document.cookie.split(";");
        for(var i=0;i < ca.length;i++) {
                var c = ca[i];
                while (c.charAt(0)==" ") c = c.substring(1,c.length);
                if (c.indexOf(nameEQ) == 0) return      c.substring(nameEQ.length,c.length);
        }   
        return "No such cookie";
      }   
      ')
    ))

#print(paste( 'cookie is ', cookie)       )


```


```{r requirements,include = FALSE}
#include = FALSE means knitr will run the chunk but not include the chunk in the final document
#  options(error=utils::recover)  # I wish this helped!

user = system('echo $USER', intern = TRUE) # for saving&retrieving files.

require(T15lumpsplit)
require(shiny)
require(xtable)

require(devtools)
if(!require(T15lumpsplit))
  devtools::install_github('professorbeautiful/T15lumpsplit')
if(!require(shinyDebuggingPanel))
  devtools::install_github('professorbeautiful/shinyDebuggingPanel')
require(magrittr);require(shinyBS)
require(knitr);require(rmarkdown);

if(interactive()) {  ### Running out of RStudio locally
  analysisReactiveSetup_DTC = paste0('inst/T15lumpsplit/', 'analysisReactiveSetup_DTC.R')
  analysisReactiveSetup_BDC = paste0('inst/T15lumpsplit/', 'analysisReactiveSetup_BDC.R')
} else  { ### Running at shinyapps.io
  analysisReactiveSetup_DTC =  'analysisReactiveSetup_DTC.R'
  analysisReactiveSetup_BDC =  'analysisReactiveSetup_BDC.R'
}
```

```{r setup numbering, warning=FALSE}
if(exists(x = 'envNumberSequences')) {
  rm(list=ls(envir = envNumberSequences), envir = envNumberSequences)
} else
  envNumberSequences = new.env()
### Note that the .GlobalEnv seen here is different from your R session... and it persists across Rmd runs!

source('initializations.R', local=TRUE)
```


```{r colors}
adviceForeground = 'darkred'
adviceBackground = 'snow'
lumpColor = 'red'
splitColor = 'blue'

## https://www.w3schools.com/cssref/css_colors.asp
```


```{r onStop, eval=FALSE}
#onStop(function() {
   #cat("Doing application cleanup\n")
#})
# Wish it worked.
```


```{r onbeforeunload}
#alert('GETTINGSTARTED')  // This works!
includeScript('www/onbeforeunload.js')   ### Not doing anything now?
```



```{r initial values, echo=FALSE}

logit <<- function(p) log(p/(1-p))
antilogit <<- function(x) 1 - 1/(1+exp(x))
DLdataOriginal = matrix(c(3,3,4,90),nrow=2)
dimnames(DLdataOriginal) = list( outcome=c("R","N"), feature=c("D","L"))
ColorForPrior="orange";     
ColorForPosterior="blue";     
ColorForLikelihood="black"
TOCnum =  0
```



```{r servercode}
DLdata = list()
rValues = reactiveValues(tau = 1,  phi = 0.001, mu=0.5,
                         title_1='title 1',
                         #DLdata=list(),
                         #DLdataMyChoice=list(),
                         #DLdataLastUsed = DLdataOriginal,
                         isResetting = FALSE,
                         isLoopingSync = FALSE,
                         WhoPriorProbProb = 1/2,
                         WhoPriorOdds = 1, #,
                         #thisSession = session
                         qStar = 1
)
```


```{r sessioninitialized, eval=FALSE}
#### This doesn't work in Rmarkdown.
invisible(
  tags$head(tags$script(
    "$(document).on('shiny:sessioninitialized',
    function(event) {
    alert('shiny:sessioninitialized  2');
    Shiny.onInputChange('sessioninitialized2', 2)
});"
  )) 
)
```


```{r shiny-connected}
### this doesn't work either.
invisible(
  tags$head(tags$script(
    "$(document).on('shiny:connected', 
  function(event) {
    alert('shiny:connected  2');
    Shiny.onInputChange('shinyconnected2', 2)
  });"
  )) 
)
```


```{r JSdoer, eval=FALSE}
### this fails.
output$JSdoer = renderUI({
  thisQANumber = 1
  outputIdThisQA = paste0('QA', thisQANumber)
  textareaIdThisQA = paste0('id', outputIdThisQA)
  linkIdThisQA = paste0('id', outputIdThisQA, "_link")
  input[[linkIdThisQA]]  ## dependency
  cat('CLICKED ', linkIdThisQA, '\n')
  shinyjs::click('downloadAllUserEntries', asis=TRUE);
  clickString =
      "document.getElementById('downloadAllUserEntries').click();"
  tags$script(clickString )
  #tags$script(paste0('eval("', clickString, '")') )
  tags$script('console.log("downloadAllUserEntries")' )
})

uiOutput(outputId='JSdoer')

```


```{r makeDebuggingPanelOutput, eval=TRUE}
observeEvent(input$ctrlDpressed, {}) # just to flush the ctrl-D press.

shinyDebuggingPanel::makeDebuggingPanelOutput(
   session, toolsInitialState = TRUE,
   condition='ctrlDpressed === true')


#options(shiny.trace=TRUE)
```

```{r fat arrow, results='asis'}
#cat("\U2B07")
# This shows you can get a fat arrow... but not in a plot.
```

```{r zoomRange}
    #### zoomAdvice ####
zoomRange <<- c(1200, 1400)
```


```{r zoomAdvice}
inclRmd('www/zoomAdvice.Rmd')
```
```{r browserAdvice}
inclRmd('www/browserAdvice.Rmd')
```


```{r loadFromCookies-Setup, results='hide'}
#'hide' prevents printing NULL, does not hurt cookies.
writingCookiesIsOK <<- FALSE
tags$head(tags$script("Shiny.onInputChange('cookieinput', document.cookie);"))
## This only works the once. But if run from the shinyDebuggingPanel JS box, it works immediately.
# But again, only once.
```


```{r in-progress, eval=FALSE}
h3("NOTE: this is a work in progress!")

actionButton("button", "Click me")
div(id = "hello", "Hello!")
```

```{r actions, eval=FALSE}
# No longer needed
observeEvent(input$button, {
 toggle("hello") # this is working now.
})
selectInput("idPageCol", "Colour:",
            c("white", "yellow", "red", "blue", "purple"))
observeEvent(input$idPageCol, {
  js$pageCol(input$idPageCol)
})
#print( paste('TQ22222222', #session$sendCustomMessage(type="readCookie",
#               message=list(name='TQ22222222')) ) )

#jsCode <- "shinyjs.pageCol = function(params){return 12341;}"
#extendShinyjs(text = jsCode, functions = c("pageCol"))
#cat('jsCode ', js$pageCol(0))
# This code works in shinyDebuggingPanel:
#  $('body').css('background', 'green')
# But all output appears in an alert window,
# cannot grab into R.
```

<a name='section-debugging'> </a>

```{r withDebuggingPanel, eval=TRUE}

fluidRow(
shinyDebuggingPanel::withDebuggingPanel()
)
```

```{r JS and CSS includes}
includeScript('www/KeyHandler.js')
includeScript('navigateToId.js')   ### ESCAPE key to return.  
###The Chrome problem is NOT:
# the order of these includeScripts.
# due to navigateY. That works in JS console.

includeCSS('TOC.css')    
### includeCSS doesn't work early on.
### It worksin the YAML.
includeCSS('lumpsplit.css')
#includeScript('www/jquery-3.3.1.min.js')

```

```{r topics in development}
## inclRmd('topics in development.Rmd')
##  Not helpful.
```


```{r UI_begins}
require(shinyBS)
```
*Package:T15lumpsplit  `r packageVersion('T15lumpsplit')`  `r packageDate('T15lumpsplit')`     Author: Roger Day, University of Pittsburgh.  Started on `r date()` *


# OVERVIEW of this Module  <!-- #title -->


## Contents of this Module  <!-- #title -->

```{r Overview-topic}
conditionalPanelWithCheckbox(
  labelString = 'Overview of the topic', 
  filename = 'Overview.Rmd', 
  initialValue = TRUE)
```

For a refresher on the concepts of bias, variance, and mean squared error in estimation, open this panel.

```{r bias-variance-meansquarederror-MSE.pdf}
conditionalPanelWithCheckboxPDF(labelString='Concepts: bias, variance, and mean squared error in estimation',
     filename="bias-variance-meansquarederror-MSE.pdf",
     cbStringId='cbStringIdBiasVariance')
```

The general scope of the bias-variance axis in many settings is here:

```{r biasVariancePicture}
#### THIS WORKS. In the Preview window, iframe is turned into external to Preview  app. but not from the browser, that's OK. ####

#### bias-variance axis ####
conditionalPanelWithCheckboxPDF(labelString='The bias-variance axis',
     filename="Reproducibility-lump-split-page-1.pdf",
     cbStringId='cbStringIdReproducibility')

```

## Tips on using this Module  <!-- #title -->


```{r Overview-editing}
inclRmd('dataEditingAdvice.Rmd')
conditionalPanelWithCheckbox(
  labelString = 'ADVICE on editing data', 
  html = uiOutput('advice_editing_data'), 
  initialValue = FALSE)
```

```{r Overview-saving-retrieving}
inclRmd('contentsAdvice.Rmd')
conditionalPanelWithCheckbox(
  labelString = 'ADVICE on saving and retrieving your answers questions and comments', 
  html =  uiOutput('contentsAdviceOutput'),
  initialValue = FALSE) 
```


```{r listings}
# ls()
# cat('-=-=-=-=\n')
# ls(pos=1)
# cat('-=-=-=-=\n')
# so it has its own .GlobalEnv, and it's persistent across knits.
# wrapperToGetKeys  ### interesting! that's mine! from DebugTools
```



# "Lumping" versus "splitting" in science. <!-- #title -->


In science, advances often proceed by "splitting": things that seem initially the same are classified as different. The classification may be tentativedistinction, until it proves to be useful. 
Other advances proceed by "lumping":  identifying things that seem different but have a deep connection.

So, we split birds and bats, but we lump bats and whales because they are both mammals. At least in this case, for some purposes, lumping supercedes splitting.
(But if studying modes of  locomotion, the lumping will be different.)

In medicine both splitting and lumping are important.

Cancer was just "cancer" before it was learned through clinical trials that some medicines work well on cancer in one organ but not another--- and vice versa for another medicine.

In recent years, however, specific molecular defects in cancer cells have "lumped" together types of cancer that we would not have dreamed of connecting.
The drug gleevec is a miracle drug for patients with chronic myelogenous leukemia (CML). But not for other leukemias. Deep insight into molecular biology of cancer showed that a radically different kind, gastrointestinal stromal tumor (GIST). 
Better lumping together with better splitting can cure cancer patients.

We will explore a simple lumping and splitting conundrum as a jumping off point for introducing a collection of important statistical ideas that connect to each other.

```{r QA-intro-lump-split}
QandAha(context='QA-intro-lump-split')
```

```{r initialize-jumpLists}
jumpList_DTC = list()
jumpList_BDC = list()
getDTCnumber = function(analysisName) {
  mapAnalysisToDTCnumber[analysisName]
}
getBDCnumber = function(analysisName) {
  mapAnalysisToBDCnumber[analysisName]
}
```

# Hypothetical medical study <!-- #title -->

## Description of the challenge <!-- #title -->

**The Problem**:  A new treatment is given to `r sum(DLdataOriginal)` patients.  
Of them, only `r sum(DLdataOriginal["R", ])` respond.  The outcome we call $Y$.  A responding patient has $Y=R$; non-reponding, $Y=N$.

That is discouraging. 

But there is a subgroup of just `r sum(DLdataOriginal[ , "D"])` in which `r sum(DLdataOriginal["R","D"])` patients respond, yielding a response rate of `r round(100*sum(DLdataOriginal["R","D"])/sum(DLdataOriginal[ , "D"]) )`%. For now we call them "$D$ patients", and the others "$L$ patients". Which subgroup a patient belongs to is a "feature" we call $X_{DL}$, with possible values $D$ or $L$. 

This feature $X_{DL}$ might be a valuable biomarker. Or it might be nonsense.
Possibilities for the actual identity of $X_{DL}$ will be explored later.

**The Challenge**: decide whether to treat the "$D$ patients" -- or more modestly, to decide whether to put resources into studying the treatment further for "$D$ patients". To guide the decision, we need to assess what we know about the **conditional probability**, probability that the patient responds to treatment given that the patient is a $D$ patient. We can see this conditional probability written different ways, some long, some short, using the vertical bar or the dot to say *"conditional on"*:

* Probability of response given a $D$ patient
* $Pr(Y~=~R ~|~ X_{DL} ~=~ D)$
* $Pr(R ~|~ D)$
* $P_{R~.~D}$



## Simple "Lump" and "Split" estimates of $Pr(R ~|~ D)$ <!-- #title -->
```{r analysis-plotLumpSplitPoints}
jumpList_DTC = c(jumpList_DTC, plotLumpSplitPoints='Plot of lump & split points')
source('analysisInitialSetup_DTC.R', local=TRUE); a(name=paste0('section-a_', analysisName) )
#cat("After first analysisInitialSetup_DTC.R, getDTCnumber('plotLumpSplitPoints') =",
#    getDTCnumber('plotLumpSplitPoints'), '\n' )
estimates_Lump_Split = observeEvent( 
  eventExpr = getDLdata('plotLumpSplitPoints'),
  {
    analysisName = 'plotLumpSplitPoints'
  source(analysisReactiveSetup_DTC, local=TRUE)
  #thisData = getDLdata('plotLumpSplitPoints')
  rValues$estimates_Lump_Split = list(
    lump = round(digits=2, sum(thisData['R', ])/
      sum(thisData[ , ])),
    split =  round(digits=2, (thisData['R', 'D'])/
      sum(thisData[ , 'D'])),
    confInterval_Split = binom.test(x = thisData['R', 'D'],
                                    n = sum(thisData[ , 'D']) )$conf.int,
    confInterval_Lump = binom.test(x = sum(thisData['R', ]),
                                   n = sum(thisData[ , ]) )$conf.int
  )
})
 
output$confInterval_Lump = renderText(
  {
  analysisName = 'plotLumpSplitPoints'
  source(analysisReactiveSetup_DTC, local=TRUE)
    try(silent = TRUE,
        paste("Dr. Lump:  ", rValues$estimates_Lump_Split[['lump']],
              "  (", 
          signif(digits=2,
              rValues$estimates_Lump_Split[['confInterval_Lump']][1]),
          "-",
          signif(digits=2,
              rValues$estimates_Lump_Split[['confInterval_Lump']][2]),
          ")"
    ) )
  })
output$confInterval_Split = renderText(
  {
      analysisName = 'plotLumpSplitPoints'
      source(analysisReactiveSetup_DTC, local=TRUE)
      try(silent = TRUE,
          paste("Dr. Split: ", rValues$estimates_Lump_Split[['split']],
              "  (", 
                signif(digits=2,
                       rValues$estimates_Lump_Split[['confInterval_Split']][1]),
                "-",
                signif(digits=2,
                       rValues$estimates_Lump_Split[['confInterval_Split']][2]),
          ")"
    ) )
  }) 

output$plotLumpSplitPoints = renderPlot(
  height = 300, 
  {
    analysisName = 'plotLumpSplitPoints'
    source(analysisReactiveSetup_DTC, local=TRUE)
    #cat('plotLumpSplitPoints: renderPlot', paste(thisData), '\n')
    #cat('plotLumpSplitPoints: renderPlot', paste(getDLdata(analysisName)), '\n')
    bivariateNormResults = #rValues$bivariateNormResults <<-
      calculatePlightPdarkPosterior(
        DLdata=thisData,
        tau=ifelse(is.null(input$tau), 1, input$tau),
        phi=ifelse(is.null(input$phi), 1, input$phi),
        mu0=ifelse(is.null(input$mu0), 0.5, input$mu0),
        fudgeFactor=ifelse(is.null(input$fudgeFactor), 1e-6, input$fudgeFactor)
      ) 
    #cat('plotLumpSplitPoints: ', names(bivariateNormResults), '\n')

    plotPlightPdarkPosterior(
      DLdata=thisData,
      bivariateNormResults=bivariateNormResults,
      showPrior = FALSE, 
      showPosterior=FALSE, showLikelihood=FALSE,
      showS = TRUE,
      showL = TRUE,
      showW = FALSE)
  })
```

Two colleagues on the study hold some strong but opposite opinions.

**Dr. Split** speaks:  “I believe that $X_{DL}$  is an important *biomarker*-- so imporant that the $D$ people and the $L$ people are fundamentally different. Since the question is what to do with $D$ people, I will only look at their data.”  

**Dr. Lump**   disagrees!   “No no, I am sure that this so-called *'biomarker'* $X_{DL}$  is irrelevant! I will ignore it, and look at the data for *all* the patients. My estimate will be more accurate.” 

Examine the data table below. 

(Throughout this module, you can change the numbers in the table and observe the results. Be sure to press "reset" when done.)

*Dr. Split* will *"split"* the sample of patients into the two groups, estimating the response rate `Pr(R | D)` by utilizing the D patients only, is 
`r reactive(round(100*rValues$estimates_Lump_Split$split))`*%*. 

*Dr. Lump* will  *"lump"* together all the patients, estimating that the response rate `Pr(R | D)` for $D$ patients is the same as `Pr(R)`, utilizing the whole set of patients, which is `r reactive(round(100*rValues$estimates_Lump_Split$lump))`*%*. 
  
In the following plot, we see the estimate and the confidence interval for `Pr(R | D)`, depending on whether the *"Lump"* or *"Split"* approach is taken. The confidence interval for *"Split"* is much wider.


```{r plotLumpSplit-UI}
fluidRow(
  column(4, 
         br(),
         div(style='text-align:center;font-weight: bold', 
             "Two estimates for Pr(R | D),",
             br(),
             "with confidence intervals :"),
         tagAppendAttributes(style=paste0("color:", splitColor,
                                          "; text-align:center"),
                             textOutput('confInterval_Split')),
         tagAppendAttributes(style=paste0("color:", lumpColor,
                                          "; text-align:center"),
                             textOutput('confInterval_Lump')),
         plotOutput('plotLumpSplitPoints')
  ),
  column(8,
         dataTableComponent(showhide='show', analysisName=analysisName))
)
```

Lumping gives an answer with low variance ($N$=100 initially; the sample size is fairly high) but high bias-- because the answer reflects far more $L$ patients than $D$ patients. So we get a precise answer to what might be the wrong question. The confidence interval follows the diagonal of the plot on the left above, where $Pr(R~|~D)=Pr(R~|~L)$.

Splitting gives an answer with high variance ($N$=6 initially) but low bias-- because it is including only patients who ARE in the $D$ group:  it is directly asking the right question, but with little precision. The very wide horizontal confidence interval reflects this high variance.

### <a name='section-ClassicalTest'> Classical estimation bifurcating on a hypothesis test</a> <!-- #title -->


```{r panelOfBifurcation}

output$fisherResult = renderUI({
  analysisName = 'plotLumpSplitPoints'
  source(analysisReactiveSetup_DTC, local=TRUE)

  Pvalue = fisher.test(thisData)$p.value
  #print(thisData)
  #print(DLdataOriginal)
  #print(identical(DLdataOriginal, thisData))
  if(identical(DLdataOriginal, thisData))
     "" ### output nothing
     else
       span(
    'Since  you have changed the data in the table above, the P value is now P = ' ,
    strong(em(signif(digits=3, Pvalue))), ".")
})
output$fisherResultOriginal = renderText({
  Pvalue = fisher.test(DLdataOriginal)$p.value
  paste0(
    '   P = ', signif(digits=3, Pvalue), '\n')
})

```

A time-honored but fading technique is to use a classical hypothesis test, to guide which of two analyses "Lump" or "Split", to present.
Here the hypothesis to be tested is " $X_{DL}$ is independent of Y";  the response rate of D patients does not differ from L patients.  Then, depending on the hypothesis test result, Dr.Lump's answer or Dr. Split's answer is selected. 

One test frequently performed to test independence in a 2x2 table like this one is the *Fisher exact test*. On the original data, the P value is 
**`r textOutput('fisherResultOriginal', inline=TRUE)`.** 

 `r uiOutput('fisherResult')`
`r br()`


If the P-value is "significant" (which "by tradition" means "less than 5%"), the "null hypothesis" (that "Dr. Lump" is correct) is rejected. With the original data, the traditional consequence is that we use Dr. Split's estimate:  60%...  and WE COMPLETELY IGNORE THE DATA FOR THE "L" PATIENTS!

```{r QA-fisher}
QandAha('QA-fisher')
```

## Parametrizations of the probabilities: notation <!-- #title -->

To proceed to deeper approaches, we need some notation.

<!-- #### parametrize.Rmd #### -->
```{r child = 'Rmd-text-snippets/parametrize.Rmd'}
```

```{r Why focus on this parameter}
TextQuestion('Why focus on this parameter?')
```


```{r QA-parametrization}
QandAha(context='QA-parametrization')
```

## Approaches to estimating Pr(R | D) <!-- #title -->

* [Classical test: is $X_{DL}$ independent of the outcome Y?](#ClassicalTest)
* [Bayes mixture:  "Dr. Who"](#Bayes_mixture)
* [Bayes joint prior, logit scale](#Bayes_joint_prior).  
* Approaches used in the famous ECMO data set. (ToDo)


### <a name='section-Bayes_mixture'> Approach: Dr. Who's Bayes mixture of "Lump" and "Split"</a> <!-- #title -->

Dr. Who doesn't know *who* to believe.


```{r child = 'DrWho.Rmd'}
```

```{r QA-DrWho}
QandAha('QA-DrWho')
```

###  Approach: Optimizing the mixture with cross-validation <!-- #title -->

Cross-validation is a technique in which the performance of a model can be checked as if on an independent dataset, without actually having a separate dataset. *$K$-fold cross-validation* divides the dataset into K disjoint subsets of observations, and loops over these sets. Each subset is, in turn, used as a `test set`. We remove it from the full dataset, re-build the model on the reduced dataset (the `training set`), and evaluate predictions of that model on the removed observations in the test set. Performance metrics are usually averaged across the $K$ repetitions. When $K=N$, the size of the full dataset, the procedure is called `leave-one-out cross-validation`, since each test set is of size one.

For this example dataset, we perform leave-one-out cross-validation as follows:

```{r Lump-Split-crossvalidation.R}
jumpList_DTC = c(jumpList_DTC, crossvalidationPlot='Cross-validation plot')
source('analysisInitialSetup_DTC.R', local=TRUE); a(name=paste0('section-a_', analysisName) )

source('Lump-Split-crossvalidation.R', local=TRUE)  # , echo = FALSE)

fluidRow(
  column(4, 
         plotOutput('crossvalidationPlot')),
  column(8,
         br(),
         dataTableComponent(analysisName='crossvalidationPlot'))
)
```

```{r TQ and QA for crossvalidation}
TextQuestion("What advantage does the cross-validation approach here have, relative to the approaches of Drs. Split, Lump and Who?")
QandAha('QA-for-crossvalidation')
```

### <a name='section-Bayes_joint_prior'> Approach: Bayes joint prior, logit scale.</a> <!-- #title -->

Another Bayesian approach is to set a Bayesian prior on the joint distribution of the two conditional probabilities Pr(R | D) and Pr(R | L). This allows data on the two probabilities to be shared between them, smoothly instead of a crude mixture of the two extreme views of   "Dr. Lump" and "Dr. Split".
```{r QA-smooth-joint-prior}
QandAha('QA-smooth-joint-prior')
```
Once we have this joint prior distribution, we compute the joint posterior.

To set the prior on the joint distribution of Pr(R | D) and Pr(R | L), we first convert them to logits ("logit" means "log(odds)").
```{r Details about the logit}
conditionalPanelWithCheckbox('Details about the logit', filename = 'www/logit.Rmd')
```


Then we set a bivariate normal distribution on the logits.

We also convert the observed proportions to logits. We estimate the variances using the delta method. Details are here:
```{r}
conditionalPanelWithCheckboxPDF(
  labelString="The delta method", 
  filename='www/the-delta-method.pdf',
  cbStringId='delta_method')
```

We can apply the delta method to the Poisson distribution, useful for count data like the data in our table. The variance of the logarithm of a count is roughly the reciprocal of the count.

(This doesn't work well if the count is near zero, and not at all at zero.
For this reason a "continuity fudge" is applied, if necessary.)

```{r}

output$postmean.p = renderText(
  try(silent = TRUE, ## Looks ok despite initial error message
      paste0(
        signif(digits=2, rValues$bivariateNormResults_bivPlot$postmean.p[1]),
        ' (95% confidence interval: ', 
        signif(digits=2, rValues$bivariateNormResults_bivPlot$confints.p[1 , 1]),
        '-',
        signif(digits=2, rValues$bivariateNormResults_bivPlot$confints.p[2 , 1]),
        ')'
      )
  )
)
```

Finally, Pr(R | D) marginalized over Pr(R | L) has posterior mean =  `r  (textOutput('postmean.p', inline=TRUE))` .

```{r linkoutbivariate}
conditionalPanelWithCheckboxPDF(
  labelString='Derivation of the bivariate normal posterior distribution', 
  filename='./lump,split-bivariate-normal-derivation.pdf', 
  cbStringId = 'bivnormPDF')

# observeEvent(eventExpr = {input$bivnorm},
#              { 
#                  linkout(
#                    './lump,split-bivariate-normal-derivation.pdf') 
#              }
# )
```


```{r QA-joint-posterior}
TextQuestion("Your reactions to the derivation?")
QandAha('QA-joint-posterior')
```


```{r BayesLogitPlot}
jumpList_DTC = c(jumpList_DTC, bivariateContourPlot='Bivariate contour plot')
source('analysisInitialSetup_DTC.R', local=TRUE); a(name=paste0('section-a_', analysisName) )

source('bivariateContourPlotReactive.R', local=TRUE)

div(
  fluidRow(
    column(4, 
           tagAppendAttributes(
             div(
               uiOutput(outputId="title_1_ID"),
               uiOutput(outputId="title_2_ID"),
               uiOutput(outputId="title_3_ID")
             ),
             style=
               'text-align:center; font-size:small;
                    font-weight:bold'),
           plotOutput(outputId = 'thePlot', height=260)
    ),
    column(8,
           br(),
           dataTableComponent(analysisName='bivariateContourPlot')
    )
  ),
  fluidRow(
    column(4, 
           ContoursPanelLegend
    ),
    column(8,
           panelOfInputs
    )
  )
)
```


### Summary table of estimates  <!-- #title -->

Assembling the results of the approaches to estimating Pr(R | D):

```{r tableOfEstimates}
jumpList_DTC = c(jumpList_DTC, estimateTable='Table of estimates')
source('analysisInitialSetup_DTC.R', local=TRUE); a(name=paste0('section-a_', analysisName) )

output$estimateTable = 
  renderTable(rownames = TRUE, {
    analysisName = 'estimateTable'
    
    source(analysisReactiveSetup_DTC, local=TRUE);
      ###  Use the last-data for everything.
    
    isolate( {
      #cat(paste(thisData), '\n')
      #cat(paste(DLdataLastUsed), '\n')
      #cat('Copying thisData to other analyses:\n')
      for(aN in 1:analysisNumber) {
        #cat('  =>',  names(mapAnalysisToDTCnumber) [aN], ':  ')
        setDLdata(value=thisData, myChoice=TRUE,
                  names(mapAnalysisToDTCnumber) [aN] )
        #cat(paste(getDLdata(myChoice=TRUE, 
        #                    names(mapAnalysisToDTCnumber) [aN]) 
        #), '\n' )
      }
    })
    nRD=thisData['R','D']
    nND=thisData['N','D']
    nR=sum(thisData['R', ])
    nN=sum(thisData['N', ])
    ## Raw results
    LumpRawMean=(nR)/(nN)
    SplitRawMean=(nRD)/(nRD+nND)
    
    ### Dr. Who's results.
    ###  Only for the flat priors. We don't allow Split and Lump priors to change.
    posterior.mean.s = (1+nRD)/(2+nRD+nND)
    posterior.mean.l = (2+nR)/(4+nN)
    pProbSplit = posteriorProb(theData=thisData)
      ### Is this responsive to user's Who prior?
    pProbLump = (1-pProbSplit)
    posterior.mean.w = 
      pProbSplit * posterior.mean.s +
      pProbLump * posterior.mean.l
    
    ## CV results
    setDLdata(value=thisData, analysisName='crossvalidationPlot')
    CVanalysisResult = doCVanalysis(thisData)
    CVoptimalEstimate = CVanalysisResult$CVoptimalEstimate
    CVoptimalWeight = CVanalysisResult$optimalWeight

    ### Bivariate logit method results.
    #setDLdata(value=thisData, analysisName='bivariateContourPlot')
    
    bivariateNormResults = isolate({
      calculatePlightPdarkPosterior(
        thisData, 
        tau=input$tauInput,
        phi=input$phiInput,
        mu0=input$mu0Input
      )
    })
    rValues$bivariateNormResults_bivPlot = bivariateNormResults
    #cat("table of estimates: names(bivariateNormResults) ",
    #    paste(names(bivariateNormResults)), '\n')
    implicitWeight1_ForLogit = 
      (bivariateNormResults$postmean.p[1] - LumpRawMean)/
      (SplitRawMean - LumpRawMean)
    implicitWeights_ForLogit = 
      c(implicitWeight1_ForLogit, 1-implicitWeight1_ForLogit)
    
    niceColHeader = c(
      'Probability estimate Pr(R | D)', 'Weight (Lump)', 'Weight (Split)')
    
    result = data.frame(
      means = c(
        LumpRawMean,
        SplitRawMean,
        posterior.mean.w,
        CVoptimalEstimate,
        bivariateNormResults$postmean.p[1]
      ),
      weightsForLump = c(
        1,
        0,
        pProbLump,
        1-CVoptimalWeight,
        implicitWeights_ForLogit[2]
      ),
      weightsForSplit = c(
        0,
        1,
        pProbSplit,
        CVoptimalWeight,
        implicitWeights_ForLogit[1]
      )
    )
    result = as.data.frame(lapply(result, round, digits=2) )
    rownames(result) = c(
      'Raw proportion (Lump)',
      'Raw proportion (Split)',
      "Dr.Who mixture posterior mean", 
      "Cross-validation optimal",
      'Bivariate inverse logit of logit posterior mean')
    return( (result) )
  })


div(style='fontsize:large;', 
    h4("Posterior probabilities and weights"),
    tableOutput('estimateTable'),
    dataTableComponent(analysisName = 'estimateTable')
)
```

As you change Dr. Who's prior probability or the data values, observe the changes in this summary table. Raw (Lump) & Raw (Split)	are the table proportions taken at face value  by Dr. Lump and Dr. Split. Bayes Mean (Lump)	and Bayes Mean (Split) and Bayes Mean (Who) are Bayesian posterior means that we calculated above. Bayes Pr(Lump) and	Bayes Pr(Split) are Dr. Who's posterior probabilities that Dr. Lump or Dr. Split (respectively) is correct.

```{r QA-Who-prior-probability}
QandAha('QA-Who-prior-probability')
```


# The role of external information, explored by introducing different identities for "D" and "L". <!-- #title -->

So, when should we lump? When should we split? Should we aim to compromise, and if so, how should we strike the balance?  

In this section we explore different scenarios of external information, to see how they affect the estimation of ${Pr(R|D}$.

##	Strong prior belief that feature $X_{DL}$ is *un*important <!-- #title -->

What if D and L represent dark hair color and light hair color? "Dr. Who" may be highly skeptical that this $X_{DL}$ really affects response, and express this by choosing a prior odds that favors Dr. Lump  over Dr. Split. 
##	Strong prior belief that the feature $X_{DL}$ is important <!-- #title -->

What if D and L represent two alleles of a gene believed to affect this drug's pharmacodynamics? Or perhaps, $X_{DL} = D = dark hair$, $X_{DL} = L = light hair$, but hair color is strongly tied to ethnicity in this sample … which is strongly tied to a key genetic variant."Dr. Who" can express choosing a prior odds that favors  Dr. Split over Dr. Lump. 

You can experiment with the effects of prior belief by choosing such a value here: 

```{r WhoPriorProbNumericInput2}
WhoPriorProbNumericInput()
```

and reviewing revised results at these previous locations:
  
```{r QA-Strong-prior-belief-X_DL-important}
inclRmd('jumpBack_DTC.Rmd')
QandAha('QA-Strong-prior-belief-about-X_DL')
```

Which of the previous analyses allow you to interject prior belief on the Lump-versus-Split axis?

##	Multiple features, with some prior belief <!-- #title -->

Suppose that we have features for a hundred genes which were *previously* identified as known to affect the pharmacodynamics of the drug being tested. Our feature $X_{DL}$ happens to be one of them; otherwise unselected.

```{r modify the "classical" approach}
TextQuestion(' How would you modify the "classical" approach that chooses Lump or Split based on a hypothesis test?')
```


```{r Bayesian account for this knowledge }
TextQuestion(' How would you account for this knowledge in one of the two Bayesian approaches?')
```

##	Best feature of 100, with some prior belief <!-- #title -->

As before, suppose that we have features for a hundred genes which were *previously* identified as known to affect the pharmacodynamics of the drug being tested. But this time, our feature $X_{DL}$ is selected to be the best of the one hundred features.

This situation entails dealing with *"multiple comparisons"*. Pure chance can make a feature seem strongly associated with the response outcome when in reality it is not associated. When the best-appearing relationship is selected out of a large number, we can be fooled into thinking the strength of evidence is stronger than it really is.

*"The more questions you ask, the more wrong answers you are likely to get."*
```{r Intro to multiple comparisons}
conditionalPanelWithCheckboxPDF(
  labelString="Intro to multiple comparisons", 
  filename='Multiple comparisons- introduction.pdf',
  cbStringId='multiple_comparisons')
```


One way to handle multiple comparisons is through a classical frequentist multiple testing adjustment. This evaluates the chance of getting a P-value as low as (or lower than) the minimum of the P-values for the features. The best-known of these methods are the Bonferroni, Sidak, and permutation /randomization tests. Here are some details on these approaches: 
```{r Multiple comparisons- classic methods}
conditionalPanelWithCheckboxPDF(
  labelString="Multiple comparisons- classic methods", 
  filename='Multiple comparisons- classic methods.pdf',
  cbStringId='Multiple_comparisons_classic')
```

and here are some examples that highlight some strange and unsettling aspects of the classical methods:

```{r Multiple comparisons- disturbing examples}
conditionalPanelWithCheckboxPDF(
  labelString="Multiple comparisons- disturbing examples", 
  filename='Multiple comparisons- disturbing examples.pdf',
  cbStringId='Multiple_comparisons_disturbing')
```

Let's put those worries aside for now.

What if we add lots of features to the feature ($X_{DL}$) of our little data set? How will the methods we have used behave when applied in this "big" data set.
Let's refer to $X_{DL}$ as $X_1$, and let's generate 99 extra features $X_2 ... X_{100}$ (each with only two possible values, 0 or 1).
<!-- You can use either the original data or the data as you may have modified it. -->


```{r makeDLdataDFYesNoYes  }
#  OK, mission accomplished, example found. We need never run this again.
makeDLdataDFYesNoYes = function(DLdata=DLdataOriginal, qStar=1, upperLimit=95){
  isYesNoYes = FALSE
  seedInt = 40
  while( (! isYesNoYes) & (seedInt < upperLimit)) {
    #savedSeed = .Random.seed
    seedInt = seedInt + 1
    checkYesNoYes(seedInt)
  }
  #print(BHtable[ , 1:4])
  saveRDS(seedInt, 'seedInt.rda')
  return(seedInt)
}
checkYesNoYes = function(seed, qStar=1) {
      #DLdataDFResult = makeDLdataDF(DLdata = DLdata)
    allChisqPs = generateAllPvalues(
      makeBigDataWithFeatures(seed=seed) )$allChisqPs
    BHtable = makeBHtable( allChisqPs, qStar )
    isYesNoYes = identical(
      as.vector(BHtable['P<cutoff', 1:3]),
                             c('yes', 'no', 'yes') )
    cat(0+isYesNoYes, seedInt, '\n')
    return(BHtable)
}
```

```{r makeDLdataDF  }
## Make DLdata into a data frame.
makeDLdataDF = function(DLdata=DLdataOriginal) {
  #print(DLdata)
  DLdataDF = 
    data.frame(
      outcome = rep(c("R","N", "R","N"),
                    times=c(DLdata)),
      X_DL = rep(c("D","D", "L","L"),
                 times=c(DLdata))
    )
  return(DLdataDF)
}
## Combine DL feature with extra features. 
makeBigDataWithFeatures = function(DLdata = DLdataOriginal,
                                  ## Make extra features.
                                  NextraFeatures = 99, beta1=10, beta2=10,
                                  Omega = 0,
                                  seed){
  DLdataDF = makeDLdataDF(DLdata)
  if(! missing(seed))
    set.seed(seed)
  extraFeaturesProbs_R <<- rbeta(NextraFeatures, beta1, beta2)
  extraFeaturesOdds_R <<-
    extraFeaturesProbs_R/(1-extraFeaturesProbs_R)
  
  if( abs(Omega) < 1e-9) {
    oddsratios = 1
    extraFeaturesProbs_N <<- extraFeaturesProbs_R
    extraFeaturesOdds_N <<- extraFeaturesOdds_R
    extraFeatures = as.data.frame(
      lapply(extraFeaturesProbs_R,
             rbinom, n=sum(DLdata),size=1)
    )
  } else {
    oddsratios <<- exp(rnorm(NextraFeatures, 0, sqrt(Omega)))
    extraFeaturesOdds_N <<- extraFeaturesOdds_R * oddsratios
    extraFeaturesProbs_N <<-
      extraFeaturesOdds_N/(1+extraFeaturesOdds_N)
    #plot(extraFeaturesProbs_N, extraFeaturesProbs_R)
    extraFeatures_N = sapply(extraFeaturesProbs_N,
                             rbinom, n=sum(DLdataDF$outcome=='N'),size=1)
    extraFeatures_R = sapply(extraFeaturesProbs_R,
                             rbinom, n=sum(DLdataDF$outcome=='R'),size=1)
    extraFeatures =
      matrix(NA, ncol=NextraFeatures, nrow=nrow(DLdataDF))
    extraFeatures[DLdataDF$outcome=='N', ] = extraFeatures_N
    extraFeatures[DLdataDF$outcome=='R', ] = extraFeatures_R
  }
  colnames(extraFeatures) = 
    paste0('X', 1+(1:NextraFeatures))
## Combine DL feature with extra features. 
  DLdataWithFeatures <<- cbind(DLdataDF, extraFeatures ) 
  return(DLdataWithFeatures)
}

#debugonce(makeBigDataWithFeatures)

## generateAllPvalues.
generateAllPvalues = function(
  BigData = makeBigDataWithFeatures()) {
  allFisherPs <<- 
    sapply(BigData[-1], function(feature) 
      fisher.test(feature,
                  BigData$outcome)$p.value
    )
  options.saved = options(warn=-1)
  allChisqPs <<- 
    sapply(BigData[-1], ## removing the outcome column 
           function(feature) {
             result = chisq.test(feature,
                        BigData$outcome)
             # if(max(result$expected) < 5)
             #   result = chisq.test(feature,
             #            DLdataWithFeatures$outcome, simulate.p.value = TRUE)
             result$p.value
           }
    ) 
  options(options.saved)
  return(invisible(
    list(allFisherPs=allFisherPs,
         allChisqPs=allChisqPs))
  ) 
}

#debugonce(generateAllPvalues)


makeSureOmegaIsGood = function(Omega) {
  Omega = as.numeric(Omega)
  if( ! (length(Omega)==1) )
    Omega = 0
  if(is.na(Omega)  | is.nan(Omega) )
    Omega = 0
  return(Omega)
}
```

```{r}

BigDataOriginal = makeBigDataWithFeatures(DLdataOriginal, seed=45)
#print(summary(DLdataDFwithFeatures_original))
allPvalues = generateAllPvalues()
allFisherPs_original = allPvalues$allFisherPs
#print(summary(allFisherPs_original))
allChisqPs_original = allPvalues$allChisqPs
#print(summary(allChisqPs_original))
# BHtable_original = DLdataDF_originalResult$BHtable
```

```{r Pvalues all features}
# observer_Pvalues_all_features = observeEvent(rValues$DLdataDFwithFeatures, {
#              Pvalues = generateAllPvalues(
#                BigData = rValues$DLdataDFwithFeatures
#              )
#              allChisqPs <<- allChisqPs = Pvalues$allChisqPs
#              allFisherP <<- rValues$allFisherPs = Pvalues$allFisherPs
# })

# DLdata = rValues$DLdata, qStar=rValues$qStar)
#     rValues$DLdataDF = makeDLdataDFResult$DLdataDF
#     rValues$allFisherPs = makeDLdataDFResult$allFisherPs
#     allChisqPs = makeDLdataDFResult$allChisqPs
#     allFisherPs <<- rValues$allFisherPs
#     allChisqPs <<- allChisqPs
#   })

```

Now that we have a data set with lots of features, let's do Fisher exact tests and chisquare tests to all the features. The red "X" is of course our original data feature, with $X_{DL}$ = D or L.



```{r pvaluesPlots}
jumpList_BDC = c(jumpList_BDC, pvaluesPlots='P-value plots')
source('analysisInitialSetup_BDC.R', local=TRUE); a(name=paste0('section-a_', analysisName) )
```



```{r pvaluesPlots-react}
output$pvaluesPlots = renderPlot({fun.pvaluesPlots()})
fun.pvaluesPlots = function() {
  analysisName = 'pvaluesPlots'
  source(analysisReactiveSetup_BDC, local=TRUE)
  #hist(rValues$allFisherPs)
  #head(sort(allFisherPs))
  #hist(rValues$allChisqPs)
  #head(sort(allChisqPs))
  plot(allChisqPs, allFisherPs, log='xy')
  points(allChisqPs[1], allFisherPs[1],
         pch='X', col='red', cex=2)
} 

#debug(fun.pvaluesPlots)
plotOutput('pvaluesPlots')
```

For each of these features $X_2 ... X_{100}$, the disparity between the two groups $X_j = 0$ and $X_j = 1$ can be represented as the odds ratio:

\[\frac{{odds(Y = R|{X_j} = 0)}}{{odds(Y = R|{X_j} = 1)}} = \frac{{\Pr (Y = R|{X_j} = 0)/\Pr (Y = N|{X_j} = 0)}}{{\Pr (Y = R|{X_j} = 1)/\Pr (Y = N|{X_j} = 1)}}\]

Initially the extra 99 feature are all "null hypotheses". The odds ratios all equal 1, so the odds that $Y=R$ does not change with $X_j$. But we can explore what happens if they do not all equal 1.
You can control Omega, the variance of logs of the odds ratios below. When Omega equals zero, the odds ratios all equal 1. Otherwise, they will spread out, some bigger than 1 and some smaller.


```{r }
bigDataComponent(analysisName=analysisName)
```

Applying the *"Bonferroni correction"* to the Fisher P value for our feature $X_{DL}$ means multiplying the P value `r signif(digits=4, allFisherPs_original[1])` by 100, to get P=`r signif(digits=4, 100* allFisherPs_original[1])`. So the evidence that the `D` people really are different is minimal at best. (To say it carefully, if a 'cutoff' for defining 'statistically significant' happened to be chosen at `r signif(digits=4, 100* allFisherPs_original[1])`, then our result for $X_{DL}$ would be just on the edge of 'significance'. That's not a compelling cutoff!) 

In the classical statistics spirit, we would now go with Dr. Lump instead of Dr. Split. (Choosing instead the chisquare test, the Bonferroni correction gives us P=`r signif(digits=4, 100* allChisqPs_original[1])`).


```{r}
# As before, you have the opportunity to play around with changing the data, or reverting to the original data set.
# dataTableComponent(showhide='hide', analysisName='pvaluesPlots')
# We probably will not put dataTableComponent in the K=100 portion.
```


```{r QA-multiple-comparisons-corrections}
QandAha('QA-multiple-comparisons-corrections')
```

## The Benjamini-Hochberg method <!-- #title -->

Another classical approach, the Benjamini-Hochberg method, is widely used in high-throughput biological assays. It is also classical frequentist method. It has a technical advantage in that the stated "false discovery rate" is accurate. However, it also has peculiar and disturbing behavior:


```{r Benjamini-Hochberg method}
conditionalPanelWithCheckboxPDF(
  labelString="The Benjamini-Hochberg method", 
  filename='Benjamini-Hochberg.pdf',
  cbStringId='Benjamini_Hochberg')
```

When applied to our augmented data set with 100 potential predictors, this is what we get, depending on qStar, the upper bound for the false discovery rate (FDR):

```{r makeBHtable}
makeBHtable = function(pValues, qStar=1, columns=8) {
  somePvalues  = 
    signif(digits=3, sort(pValues)[1:columns])
  BHtable = data.frame(
    Pvalue = somePvalues,
    cutoff = qStar * (1:columns)/100)
  BHtable[[ 'P<cutoff' ]] = c('no','yes')[
    1 + (BHtable$cutoff > BHtable$Pvalue) ]
  BHtable = t(BHtable)
  rownames(BHtable) = c('Pvalue', 'cutoff', 'P<cutoff')
  return(BHtable)
}
```

```{r output for BHtable}
jumpList_BDC = c(jumpList_BDC, BHtable='Benjamini-Hochberg')
source('analysisInitialSetup_BDC.R', local=TRUE); a(name=paste0('section-a_', analysisName) )

div(
  numericInput('qStarID', 'qStar (False Discovery Rate upper bound)',
               value = 1, min=0,  step=0.1)
)

# observeEvent( input[[get_thisOmegaID(analysisName)]], {
#   OmegaLastUsed <<- input[[get_thisOmegaID(analysisName)]]
#   cat('Setting OmegaLastUsed to ', OmegaLastUsed, '\n')
# })

output$BHtable = renderTable(
  digits=3,
  rownames=TRUE,
  expr={ 
    input$qStarID; 
    currentBDCnumber = mapAnalysisToBDCnumber['BHtable']
    thisBigDataID = paste0('BigData', currentBDCnumber)
    thisOmegaID = get_thisOmegaID('BHtable')
    thisOmega = makeSureOmegaIsGood(input[[thisOmegaID]])

    fun.BHtable()
  }
)
fun.BHtable = function(){
  analysisName = 'BHtable'
  source(analysisReactiveSetup_BDC, local=TRUE);
  tryResult = try({
    # cat('qStarID:', input$qStarID, '\n')
    as.numeric(input$qStarID)
  })
  if(class(tryResult) == 'try-error' | is.null(tryResult) |is.na(tryResult)) {
    qStar = 0
  } else qStar = tryResult
  rValues$BHtable = makeBHtable(
    allChisqPs, 
    qStar = qStar
  )
  
  #rValues$BHtable
}

#debug(fun.BHtable)
tableOutput('BHtable')
bigDataComponent(analysisName=analysisName)

```



```{r which features are significant}
TextQuestion("Using the original data, and the FDR upper bound (qStar) equal to one, which features are 'significant'? How many 'false discoveries' are there?")
```



```{r Raising qStar}
TextQuestion("Raising or lowering qStar from 1.0, how does the collection of 'significant' features change?")
```

This is a good time to record your thoughts!

```{r QA-Benjamini-Hochberg}
QandAha('QA-Benjamini-Hochberg')
```

## An Empirical Bayes approach to multiple comparisons <!-- #title -->

Another approach is through *empirical Bayes*, a method that presumes that the associations of the features with the outcome are drawn from a distribution. This allows a kind of sharing of information reminiscent of the sharing of information between the $X_{DL}$=D and $X_{DL}$=L patients, but sharing across features instead of across subgroups defined by one feature. 

The following link develops the empirical Bayes idea in the context of medical decision-making. When done reading this, close the panel, record your Questions and Aha's, and continue to see the application of EB for our example.

```{r Introduction to Hierarchical Bayes and Empirical Bayes}
conditionalPanelWithCheckboxPDF(
  labelString="An Introduction to Hierarchical Bayes and Empirical Bayes", 
  filename='www/Intro_to_Hierarchical_Bayes_and_Empirical_Bayes.pdf',
  cbStringId='EmpiricalBayes')
```

We can apply EB to a collection of estimates. In this instance, let's collect the estimates of association between the outcome and all 100 features. These estimates are the odds ratios. For the original feature $X_{DL}$, the odds ratio is
`r DLdataOriginal['R','D']` / `r DLdataOriginal['N','D']` * `r DLdataOriginal['R','L']` / `r DLdataOriginal['N','L']`), which is 
`r oddsRatioOriginal = DLdataOriginal['R','D'] / DLdataOriginal['N','D'] * DLdataOriginal['R','L'] / DLdataOriginal['N','L'] ` ).

This can be compared to all 100 odds ratios for the features:

```{r oddsratiofunctions}
  fun.oddsratio = function(col, outcome, sprinkle=1) {
    theTable = table(col, outcome)
    theTable = sprinkle + theTable
    return(theTable[1,1]/theTable[2,1]
           /theTable[1,2]*theTable[2,2])
  }

  fun.logitRatioVariance = function(col, outcome, sprinkle=0.1) {
    theTable = table(col, outcome)
    theTable = sprinkle + theTable
    sum(1/theTable)
  }
```

```{r oddsratioplot}

jumpList_BDC = c(jumpList_BDC, oddsratioplot='Odds ratio plot')
source('analysisInitialSetup_BDC.R', local=TRUE); a(name=paste0('section-a_', analysisName) )

output$oddsratioplot = renderPlot({ fun.oddsratioplot () })
fun.oddsratioplot = function() {
  analysisName ='oddsratioplot'
  source(analysisReactiveSetup_BDC, local=TRUE)

  outcome = thisBigData$outcome
  oddsratios = 
    sapply(thisBigData[-(1)], fun.oddsratio, 
           outcome=outcome)
  logitRatios = log(oddsratios)

  logitRatioVariances =
    sapply(thisBigData[-(1)],
           fun.logitRatioVariance,
           outcome=outcome)
  logitRatioSDs = sqrt(logitRatioVariances)
  
  plot(density(logitRatios), xlab='log(oddsratios)',
       main='density smooth of log(oddsratios)')
  points(logitRatios[-1], col='red', type='h',
       logitRatioSDs[-1] * par('usr')[4]/
        max(logitRatioSDs) *0.3)
  points(x = logitRatios[1], col='green', type='h',
         y = logitRatioSDs[1] * par('usr')[4]/
           max(logitRatioSDs) *0.3,
         lwd=2)
  text(x = logitRatios[1], y = par('usr')[4], pos = 2, cex=2,
       expression( "" %<-% "DL data"), srt=90, col='green')
}
#debug(fun.oddsratioplot)
plotOutput('oddsratioplot')
bigDataComponent(analysisName=analysisName)

```


Using the delta method mentioned above, 
the logs of the odds ratios have sampling variances roughly equal to the sums of reciprocals of the counts. 
(The heights of the vertical lines are proportional to the sampling standard deviations.)
From this we can construct confidence intervals for the logits, then pull them back to confidence intervals for the odds ratios. 
So for example, for our original${X_{DL}}$ 
= `D` or `L` feature, the confidence interval is given by the R expression

`exp ( log(oddsRatioOriginal) + c(-1,1)*sqrt(sum(1/DLdataOriginal))*qnorm(0.975))`,

which for a 95% confidence level is 
`r {ci=signif(digits=3, exp ( log(oddsRatioOriginal) + c(-1,1)*sqrt(sum(1/DLdataOriginal))*qnorm(0.975))); paste(ci, collapse=' to ')}`.

To apply the EB idea, we use the data for all the features, to "shrink" all the logit ratios ${X_i}$ towards the common mean, estimated as ${\hat{~\mu}}$. This solution uses a model in which the true logit ratios are all drawn from a common parent distribution, assumed to be normal (Gaussian). Then a standard Bayesian calculation gives us the posterior means (and variances) for each logit ratio. In particular, letting ${\mu_i}$ be the logit ratio for feature $i$, with variance ${\sigma_i}$ (which we estimate from the delta method to get ${\hat{~\sigma_i}}$), the posterior mean is 

  \[E({\mu _i}|{X_i}) = {X_i}(1 - {w_i}) + \hat{~\mu} {w_i},\]
 
 where the weights $w_i$ vary with the feature:
 
  \[w_i = \frac{\hat{~\sigma_i}}{{\hat{~\sigma_i}}+\hat{~A}}.\]
  
  
$\hat{~A}$ is the estimated prior variance for all the true ${\mu_i}$.
 
This is yet another example of bias-variance trade-off. By accepting some bias in including data from other features, the variance of the estimate decreases, enough to make the net result more accurate.
 
But we would have to know the true mean and variance of the parent distribution. 

The "empirical" part of empirical Bayes is to use the data to estimate the mean and variance of the parent distribution generating the $\mu_i$. As you can see in the document in the panel just above, the "empirical" step needs the marginal distribution of the observed logit ratios:

  \[{X_i}|\tau \sim N(\mu,\tau  + {\sigma _i}),\]

where we will get the variances ${\sigma _i}$ from the delta method. The unknown $\mu$ and $\tau$ are called the "hyperparameters". We can estimate them however we prefer, but using maximum posterior estimation we get

\[\hat{~\mu}  = \sum {\left( {{X_i}{w_i}} \right)} /\sum {\left( {{w_i}} \right)}\]

where the weights are ${w_i} = 1/{\sigma _i}$. 
These are the optimal weights for smallest variance of $\hat{~\mu}$. 

As an *optional topic*, you may read the following on Lagrange multiplier method for optimization. Example (A) covers this case.

```{r Lagrange multipliers}
conditionalPanelWithCheckboxPDF(
  labelString="Lagrange multipliers", 
  filename='www/Lagrange multipliers with examples.pdf',
  cbStringId='Lagrange_multipliers')
```

Continuing the "empirical" part of EB, one estimate of $\tau$ is to take the total sums of squares and subtract the portion purely due to the individual data noise of the features. When the individual variances vary greatly, the estimate is a little more complicated. One way is via maximum likelihood. 

```{r empBayeslogitRatios}
jumpList_BDC = c(jumpList_BDC, empBayeslogitRatios='Empirical Bayes logit ratios')
source('analysisInitialSetup_BDC.R', local=TRUE); a(name=paste0('section-a_', analysisName) )

output$empBayeslogitRatios = renderPlot({ fun.empBayeslogitRatios() })
fun.empBayeslogitRatios = function(){
  analysisName = 'empBayeslogitRatios'
  source(analysisReactiveSetup_BDC, local=TRUE)
  
  oddsratios = sapply(thisBigData[-(1)], 
                      fun.oddsratio,  outcome=thisBigData[[1]])
  logitRatios = log(oddsratios)

  logitRatioVariances =
    sapply(thisBigData[-(1)],
           fun.logitRatioVariance,  outcome=thisBigData[[1]])
  logitRatioSDs = sqrt(logitRatioVariances)
  loglikFun=function(mu,tau) {
    sum(log(dnorm(logitRatios, mu, sqrt(tau+logitRatioVariances))) )
  }
  muHat = mean(logitRatios)
  tauHat = 0   #(var(logitRatios) - mean(logitRatioVariances))
  for( irep in 1:13) { # we ain't buildin' no cathedral here.
    mu.last = muHat
    muHat = optimize(f = loglikFun, tau = tauHat,
                  interval = c(-4,4), maximum=TRUE
    )$maximum
    tau.last = tauHat
    tauHat = optimize(f = loglikFun, mu = muHat,
                   interval = c(0, 1), maximum=TRUE
    )$maximum
    if(max(abs(mu.last-muHat), abs(tau.last-tauHat)) < 1e-4)
      break
  }
  
  rValues$tauEstimateEB = tauHat
  # cat('mu-hat: ', muHat, '  tau-hat: ', tauHat, '\n')
  weights = 1 - tauHat / (logitRatioVariances + tauHat)
  rValues$posteriorMeans = 
    logitRatios * (1-weights) + muHat * weights
  plot(c(-4, 3), 0:1, col='white',
       xlab= '', ylab='', axes=F,
       cex=2)
  axis(1, cex=2)
  #  axis(2, at = 0:1, labels = c("raw", "EB shrunk"), lwd = 0)
  # text(x = range(logitRatios)*1.05, y=c(0,0), 
  #      labels = c("raw", "raw"), xpd=NA, cex=2)
  # text(x = range(logitRatios)*1.05, y=c(1,1), 
  #      labels = c("EB\nshrunk", "EB\nshrunk"), xpd=NA, cex=2)
  axis(3, cex=2)
  # title(
  mtext(text = 'log of odds ratios: raw estimates ("Dr. Split" style)', 
        side = 1, line = 3, cex=2, xpd=NA)
  mtext(text = 'log of odds ratios: "shrunken" estimates with empirical Bayes', 
        side = 3, line = 2.5, cex=2, xpd=NA)
  for(iobs in 2:length(logitRatios))
    lines(c(logitRatios[iobs], rValues$posteriorMeans[iobs]),
          0:1)
  lines(c(logitRatios[1], rValues$posteriorMeans[1]),
        0:1, col='green', lwd=3)
  points(logitRatios[1], 0, 
         pch="X", col='green', cex=3)
  points(rValues$posteriorMeans[1], 1,
         pch="X", col='green', cex=3)
  lines(c(logitRatios[iobs], rValues$posteriorMeans[iobs]),
        0:1)
  posteriorVariance_DL =  max(0, 1/(
    (1-weights[1]) / logitRatioVariances[1]
     +   weights[1] / tauHat)
  )
  posteriorStDev_DL = sqrt(posteriorVariance_DL)
  alpha = 0.1
  lines(logitRatios[1]+c(-1,1)*qnorm(alpha)*
          sqrt(logitRatioVariances[1]), 
        c(0,0), col='green', lwd=3)
  lines(rValues$posteriorMeans[1]+c(-1,1)*qnorm(alpha)*
          posteriorStDev_DL, 
        c(1,1), col='green', lwd=3)
}
#debug(fun.empBayeslogitRatios)
plotOutput('empBayeslogitRatios')
```

The green horizontal lines are the 80% confidence intervals, before (below) and after (above) applying the empirical Bayes shrinking towards the mean of all the logit ratios. We see that the one above is narrower. So shrinking (which shares info among the features) makes the estimate for the log odds ratio more precise, compared to the "Dr. Split" perspective that directs us to allow only the original "D/L" feature to inform the odds ratio for that feature.

Initially, all the features are generated with an odds ratio = 1:  no relationship with response. in the graph, all the estimates are shrunk VERY hard to the grand mean of log odds ratios, which is close to log(1) = 0. 

What happens if the collection of 100 odds ratios deviates from 1? You can control the variance of the log odds ratio here. 

```{r}
bigDataComponent(analysisName=analysisName)
```

```{r tau comparison}
cat(' tau comparison: ', analysisName, ' ', mapAnalysisToBDCnumber[analysisName],
    get_thisOmegaID(mapAnalysisToBDCnumber[analysisName]),
    '\n')
output$tauEstEB = renderText({
  try(silent = TRUE, {rValues$tauEstimateEB}) 
})
thisOmegaID = get_thisOmegaID(mapAnalysisToBDCnumber[analysisName])
thisOmegaID_text = paste0(thisOmegaID, '_text')
output[[thisOmegaID_text]] = renderText({
  try(silent = TRUE, {
    signif(digits=4, getBigData_Omega(analysisName)) 
    ## the 'try' is just to protect from a meaningless error msg first time through.
  })
})
```

The estimated variance of the logits is `r  textOutput('tauEstEB', inline=TRUE)`. (The value that generated the data is `r  textOutput(thisOmegaID_text, inline=TRUE)` ) .


```{r sharing information among the features justified}
TextQuestion(
'Is this approach really an improvement here? Is this  sharing information among the features justified? What factors might make you more, or less, inclined to take this approach? ')
QandAha('QA-empBayeslogitRatios')
```
This is fine as far as it goes, but our result for the odds ratio does not directly address the original question of interest:  "What is the true response rate for `D` people?" 
So here is a challenge question:

```{r convert this estimate of the odds ratio}
TextQuestion('
How could we convert this estimate of the odds ratio for the X_DL feature into an estimate for Pr(Response | X_DL = D)? 
')
```


##  The Qvalue approach for multiple testing <!-- #title -->

We can also apply EB in quite a different way, to judge a collection of P values. One popular alternative to the Bonferroni and Sidak adjustments with an empirical Bayes flavor is the "Qvalue" method. 

This method assumes that the distribution of the observed P values is a mixture of a uniform distribution for the true null hypotheses and a distribution concentrated near zero for the true alternative hypotheses. The false discovery rate (FDR) and posterior probability of each null hypothesis come directly from Bayes Theorem. However, the mixture proportion, which is the prior probability of each null hypothesis, is unknown. The empirical Bayes idea directs us to estimate it. Here are some details.

```{r Storey-Tibshirani Qvalue method}
conditionalPanelWithCheckboxPDF(
  labelString="The Storey-Tibshirani Qvalue method", 
  filename='Storey-Tibshirani.pdf',
  cbStringId='Storey_Tibshirani'
)
```

Here we apply the qValue method to the chisquare P values from our dataset with 100 features:

```{r qvaluePlot}
jumpList_BDC = c(jumpList_BDC, qValue='Qvalue adjustment')
source('analysisInitialSetup_BDC.R', local=TRUE); a(name=paste0('section-a_', analysisName) )

output$qvaluePlot = renderPlot({ fun.qvaluePlot() })
fun.qvaluePlot = function(){
  analysisName = 'qValue'
  source(analysisReactiveSetup_BDC, local=TRUE)
  qout = rValues$qValueObject =
    qvalue::qvalue(allChisqPs)
  P_Q_for_X_DL = 
    c(qout$pvalues[1], qout$qvalues[1])
  qout.orig  =
    qvalue::qvalue(c(allChisqPs_original[1], allChisqPs[-1]) )
  P_Q_for_X_DL_original = 
    c(qout.orig$pvalues[1], qout.orig$qvalues[1])
  qout.mod  =
    qvalue::qvalue(allChisqPs )
  P_Q_for_X_DL_modified = 
    c(qout.mod$pvalues[1], qout.mod$qvalues[1])
  xlim = c(0, 0.004) #c(0, 1.1 * max(rValues$P_Q_for_X_DL_modified,
        #             sort(qout.mod$pvalues)[1:8][8]) )
  ylim = c(0, 0.04) #c(0, 1.1 * max(rValues$P_Q_for_X_DL_modified,
          #           sort(qout.mod$qvalues)[1:8][8]) )
  plot(qout.mod$pvalues, qout.mod$qvalues, 
       # xlim=xlim, 
       # ylim=ylim, 
       cex=2,
       xlab="P value", ylab='Q value', 
       log='xy')
  ## interpolate for the original? No, easy version. 
  ## allChisqPs_original[1]
  ## 
  
  points(qout.orig$pvalues[1], qout.orig$qvalues[1], 
         col='green')
  points(qout.orig$pvalues[1], qout.orig$qvalues[1], 
         pch='.', cex=20, col='green')
  text(qout.orig$pvalues[1], qout.orig$qvalues[1], adj=-0.1,
       family = "Arial Unicode MS",
       labels = expression( phantom(0) %<-% 
         X[DL] ~~ '(original data)'), col='green', cex=2)
  if(qout.mod$pvalues[1] != allChisqPs_original[1]){
    points(qout.mod$pvalues[1], qout.mod$qvalues[1], 
           col='blue')
    points(qout.mod$pvalues[1], qout.mod$qvalues[1], 
           pch='.', cex=20, col='blue')
    text(qout.mod$pvalues[1], qout.mod$qvalues[1], adj=-0.1,
         #family = "Arial Unicode MS",
         #labels = expression('\U2B05 X_DL (modified data)'), col='blue')
         labels = expression( phantom(0) %<-% X[DL] ~~ '(modified data)'), 
         col='blue', cex=2)
  }
  # ⬅︎
  # LEFTWARDS BLACK ARROW
  # Unicode: U+2B05 U+FE0E, UTF-8: E2 AC 85 EF B8 8E
  nFeatures = 100
  #abline(a=0, b=nFeatures, col='darkred')
  sidak = function(p, n) (1-(1-p)^n)
  left = par("usr")[1]   ##  10 ^ par("usr")[1:2]
  right = par("usr")[2]
  bottom = par("usr")[3]
  top = par("usr")[4]
  xvalues = seq(left, right, length.out = 100)
  lines(10^xvalues, sidak(10^xvalues, n=nFeatures), col='darkred')
  # aspect = `/`(par("usr")[2:1])/`/`(par("usr")[4:3])
  #bonfTilt = atan(nFeatures * aspect)* 180/pi
  #diagTilt = atan( aspect )* 180/pi
  text(x=10^xvalues[20], y=sidak(10^xvalues[20], n=nFeatures),
       'Bonferroni\n(Sidak)', cex=2, pos=2, # 2 = to the left
       #srt=bonfTilt,
       col='darkred')
  abline(a=0, b=1, col='blue')
  text(10^(bottom/2), 10^(bottom/2), 'diagonal', cex=2, pos=4, # 4 = to the right
       #srt=diagTilt, 
       col='blue')
  #plot(qvalue::qvalue(allChisqPs) )
}

#debug(fun.qvaluePlot)
plotOutput('qvaluePlot')
```

For our original D/L feature, with P value  `r reactive({rValues$P_Q_for_X_DL_original[1]})`, the Q-value is `r reactive({rValues$P_Q_for_X_DL_original[2]})`. 

As before, you can modify the spread of the true odds ratios here to see how the Q-value changes. 

```{r}
bigDataComponent(analysisName='qValue')

```


```{r TQ-qvalues-changing-DL-data}
TextQuestion('When you change the data set, what changes do you see, & why?')
```

```{r TQ-qvalues-changing-extra-features}
TextQuestion('When you regenerate the extra features, what changes do you see, & why?')
```

We use the package `qvalue` for this calculation, which can be obtained from https://www.bioconductor.org/packages/release/bioc/html/qvalue.html. 

```{r QA-empirical-Bayes}
QandAha('QA-empirical-Bayes')
```



## Two alleles of one gene out of a hundred thousand tested; nothing known <!-- #title -->

We won't illustrate this with data here, but think about this situation and how to approach it.

**One key point**: none of these methods utilizes *prior belief* about the *plausibility* of any or all of these features. To utilize prior belief across ALL the features, one has to develop a *joint* prior distribution across all hypotheses. This is difficult, but there has been progress.

```{r QA-big-joint-priors}
QandAha(context='QA-big-joint-priors')
```

#  Final thoughts

Thanks for trying out this material. Any thoughts about your experience with this module will be appreciated.

```{r TQ-final-thoughts}
TextQuestion('What worked well, what worked badly, what needs improvement?')
```

```{r loadFromCookies}
writingCookiesIsOK = FALSE
cleanupTextAreaText = function(x)  ## "=" seems ok.
  gsub('%20', ' ', 
       gsub('%25', '', # will be '%',
            gsub('EQUALS', '=',
            gsub('%3B', ';',
                 x=x))))
isStringOfInterest <<- function(x)
  length(grep('[a-zA-Z0-9]', x)) !=0
tags$script("Shiny.onInputChange('cookieinput', document.cookie);")
```

```{r}
observe(label = 'cookieObserver', x = {
  writingCookiesIsOK = FALSE

  print(cookieRetrieved <<- input$cookieinput)
  if (!is.null(cookieRetrieved)) {
    cookieList = strsplit(cookieRetrieved, split = ';')[[1]]
    cookieNamesAndValues = strsplit(cookieList, split = '=')
    cookieNames = gsub('^ ', '', lapply(cookieNamesAndValues, '[', 1)  )
    cookieValues <<- gsub('^ ', '',
                          lapply(cookieNamesAndValues, '[', 2)  )
    names(cookieValues) <<- cookieNames
    #print(cookieValues)
    isolate ({
      for(context in c('TQ', 'QA') ) 
      for(thisSeqNumber in
          seq(along=get(paste0(context,'_contexts')))) {
        outputIdThisOne = paste0(context, thisSeqNumber)
        textareaIdThisOne = paste0('id', outputIdThisOne)
        currentValue = as.vector(input[[textareaIdThisOne]])
        if( isStringOfInterest(currentValue)) {
            cat("box ", outputIdThisOne, ' has ==>',
                currentValue, '<==(', nchar(currentValue), ')\n')
          } #else {
        newValue = as.vector(
          cookieValues[outputIdThisOne])
        newValue = cleanupTextAreaText(newValue)
        if( isStringOfInterest(newValue)) {
          cat("newValue ", outputIdThisOne, ' has ==>',
              newValue, '<==\n')
          cat('Filling ...', newValue, '\n')
          updateTextAreaInput(
            session,
            textareaIdThisOne,
            value = newValue )
        }
        #}
        #}
      }
    })  # end of isolate
  }
  writingCookiesIsOK <<- TRUE
} )
# Resume updating cookies from text areas.
```

The code for this module is hosted at ['professorbeautiful/T15lumpsplit'](https://github.com/professorbeautiful/T15lumpsplit).

#  Save your Comments and Answers
```{r assembleComments}

### TODO:  On startup, load the existing file into the answer boxes.
assembleQAComments = reactive( {
  lastQANumber = length(QA_contexts)
  QAComments = data.frame(
    contexts = unlist(QA_contexts),
    contents = sapply(1:lastQANumber, function(num) {
      outputIdThisQA = paste0('QA', num)
      textareaIdThisQA = paste0('id', outputIdThisQA)
      return(input[[textareaIdThisQA]])
    })
  )
  QAComments
})

assembleTQAnswers = reactive({
  lastTQNumber = length(TQ_contexts)
  TQanswers = data.frame(
    contexts = unlist(TQ_contexts),
    contents = sapply(1:lastTQNumber, function(num) {
      outputIdThisTQ = paste0('TQ', num)
      textareaIdThisTQ = paste0('id', outputIdThisTQ)
      return(input[[textareaIdThisTQ]])
    })
  )
  TQanswers
})

assembleUserEntries = reactive({
  rbind(assembleTQAnswers(), assembleQAComments())
})

### linebreak works but not needed. renderTable will abide.
linebreak = function(s, maxchar=30) {
  wordsplit = strsplit(s, split=' ')[[1]]
  nchars = sapply(wordsplit, nchar)
  cum = cumsum(nchars)%/% maxchar
  which(cum[-length(cum)] != cum[-1])
  paste(collapse='<br>',
        sapply(split(wordsplit, cum), paste, collapse=' ')
  )
}

assembledContentsToHTML = function(contents) {
  # result = sapply(contents, function(content){
  #   return(paste(content, '<br>'))
  # })
  # result = paste(contents, collapse="<BR>")
  # print(str(result))
  # return(HTML(result))
  #contents[[1]] = linebreak(contents[[1]])
  contents
}

output$commentsToText = renderTable({
  assembleQAComments()  ### QA
  # commentsText <<- capture.output(comments)
  #writeLines(commentsText, commentsTextFile)
  # assembledContentsToHTML(commentsText)
})
output$answersToText = renderTable({
  assembleTQAnswers()   ## TQ
  # answersText <<- capture.output(answers)
  #writeLines(answersText, answersTextFile)
  # assembledContentsToHTML(answersText) 
})
```

```{r downloadHandlers}
output$downloadAnswers <- downloadHandler(
  filename = "_answers_T15lumpsplit.txt",
  content = function(file) {
    answers = assembleTQAnswers()   ## TQ
    #answersText <<- capture.output(answers)
    write.csv(answers, file, row.names = FALSE)
  }
  #, outputArgs=list()
)
output$downloadQandA <- downloadHandler(
  filename = "_comments_T15lumpsplit.txt",
  content = function(file) {
    comments = assembleQAComments()  ### QA
    #commentsText <<- capture.output(comments)
    write.csv(comments, file, row.names = FALSE)
  }
)
output$downloadAllUserEntries <- downloadHandler(
  filename = paste0("_", user, "_UserEntries_T15lumpsplit.txt"),
  content = function(file) {
    entries = assembleUserEntries()  ### QA and TQ
    #commentsText <<- capture.output(comments)
    write.csv(entries, file, row.names = FALSE)
  }
)
```

```{r assemble comments and answers}

div(
  tags$b('View and download your "Questions&Ahas" comments and "TextQuestion" answers here.'),
  div(
    #`data-display-if` = 'false',

  conditionalPanelWithCheckbox(
    labelString = 'View your comments', 
    html=tableOutput('commentsToText'),
    initialValue = FALSE  ),
  conditionalPanelWithCheckbox(
    labelString = 'View your answers', 
    html=tableOutput('answersToText'),
    initialValue = FALSE  ),
  tags$b(
    'Save these entries, to share & discuss with a mentor.')
  ),    downloadButton(outputId="downloadAllUserEntries", 
                     "Download your Q&A comments and your TQ answers here."),
  
)
```

```{r initialContentReader}
# Nope. see contentsAdvice
# dlFolder = paste0(Sys.getenv('HOME'), 'Downloads')
```

```{r browserDetect.js}
includeScript('www/browserDetect.js')
```
