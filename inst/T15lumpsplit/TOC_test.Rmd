---
title: "TOC_test"
author: "Roger Day"
date: "7/26/2021"
resource_files:
- www/zoomAdvice.Rmd
- www/zoom_triggers.js
- www/readInnerWindow.js
- www/the-delta-method.pdf
- www/BayesEquation.Rmd
runtime: shiny
output:
  html_document:
    number_sections: yes
    toc: yes
    toc_depth: 6
    toc_float:
      collapsed: no
      smooth_scroll: no
---


\[
\newcommand{\mylarge}[1] {\large{#1}\normalsize}
\newcommand{\mynormal}[1] {{\normalsize{#1}}}
\]

# title 1

# title 2

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = FALSE)
options(knitr.duplicate.label = 'allow')
```


```{r shinyjs, context="server", echo=FALSE, include=TRUE}

#shiny::addResourcePath("shinyjs", system.file("srcjs", package = "shinyjs"))
#library(shinyjs)
shinyjs::useShinyjs(html = TRUE)
require(shinyjs)
shinyjs::useShinyjs(rmd=TRUE, html=TRUE, debug=TRUE)
#<script src="shinyjs/inject.js"></script>

saveEntriesJScode =  
  "shinyjs.saveEntriesJS = function() {document.getElementById('downloadAllUserEntries').click();};"
helloJScode = 
  "shinyjs.helloJS = function() {alert('HELLO');};"
shinyjs::extendShinyjs(text = saveEntriesJScode, functions=c('saveEntriesJS'))
```



```{r include = FALSE}
#  options(error=utils::recover)  # I wish this helped!

user = system('echo $USER', intern = TRUE) # for saving&retrieving files.

require(T15lumpsplit)
require(shiny)
require(xtable)

require(devtools)
if(!require(T15lumpsplit))
  devtools::install_github('professorbeautiful/T15lumpsplit')
if(!require(shinyDebuggingPanel))
  devtools::install_github('professorbeautiful/shinyDebuggingPanel')
require(magrittr);require(shinyBS)
require(knitr);require(rmarkdown);

if(interactive()) {  ### Running out of RStudio locally
  analysisReactiveSetup_DTC = paste0('inst/T15lumpsplit/', 'analysisReactiveSetup_DTC.R')
  analysisReactiveSetup_BDC = paste0('inst/T15lumpsplit/', 'analysisReactiveSetup_BDC.R')
} else  { ### Running at shinyapps.io
  analysisReactiveSetup_DTC =  'analysisReactiveSetup_DTC.R'
  analysisReactiveSetup_BDC =  'analysisReactiveSetup_BDC.R'
}
```

```{r setup numbering, warning=FALSE}
if(exists(x = 'envNumberSequences')) {
  rm(list=ls(envir = envNumberSequences), envir = envNumberSequences)
} else
  envNumberSequences = new.env()
### Note that the .GlobalEnv seen here is different from your R session... and it persists across Rmd runs!

source('initializations.R', local=TRUE)
```
# title 3


```{r colors}
adviceForeground = 'darkred'
adviceBackground = 'snow'
lumpColor = 'red'
splitColor = 'blue'

## https://www.w3schools.com/cssref/css_colors.asp
```

```{r onStop}
#onStop(function() {
   #cat("Doing application cleanup\n")
#})
# Wish it worked.
```

```{r}
#alert('GETTINGSTARTED')  // This works!
includeScript('www/onbeforeunload.js')   ### Not doing anything now?
```




```{r initial values, echo=FALSE}

logit <<- function(p) log(p/(1-p))
antilogit <<- function(x) 1 - 1/(1+exp(x))
DLdataOriginal = matrix(c(3,3,4,90),nrow=2)
dimnames(DLdataOriginal) = list( outcome=c("R","N"), feature=c("D","L"))
ColorForPrior="orange";     
ColorForPosterior="blue";     
ColorForLikelihood="black"
TOCnum =  0
```

```{r servercode}
DLdata = list()
rValues = reactiveValues(tau = 1,  phi = 0.001, mu=0.5,
                         title_1='title 1',
                         #DLdata=list(),
                         #DLdataMyChoice=list(),
                         #DLdataLastUsed = DLdataOriginal,
                         isResetting = FALSE,
                         isLoopingSync = FALSE,
                         WhoPriorProbProb = 1/2,
                         WhoPriorOdds = 1, #,
                         #thisSession = session
                         qStar = 1
)
```


```{r sessioninitialized}
#### This doesn't work in Rmarkdown.
invisible(
  tags$head(tags$script(
    "$(document).on('shiny:sessioninitialized',
    function(event) {
    alert('shiny:sessioninitialized  2');
    Shiny.onInputChange('sessioninitialized2', 2)
});"
  )) 
)
```


```{r shiny:connected}
### this doesn't work either.
invisible(
  tags$head(tags$script(
    "$(document).on('shiny:connected', 
  function(event) {
    alert('shiny:connected  2');
    Shiny.onInputChange('shinyconnected2', 2)
  });"
  )) 
)
```


```{r makeDebuggingPanelOutput}
observeEvent(input$ctrlDpressed, {}) # just to flush the ctrl-D press.

shinyDebuggingPanel::makeDebuggingPanelOutput(
   session, toolsInitialState = TRUE,
   condition='ctrlDpressed === true')


#options(shiny.trace=TRUE)
```

```{r fat arrow, results='asis'}
#cat("\U2B07")
# This shows you can get a fat arrow... but not in a plot.
```

```{r zoomRange}
    #### zoomAdvice ####
zoomRange <<- c(1200, 1400)
```

```{r zoomAdvice}
invisible(inclRmd('www/zoomAdvice.Rmd'))
```


```{r browserAdvice}
invisible(inclRmd('www/browserAdvice.Rmd'))
```

```{r contentsAdvice}
invisible(inclRmd('www/contentsAdvice.Rmd'))
```

```{r in-progress}
h3("NOTE: this is a work in progress!")
```

<a name='section-debugging'> </a>

```{r withDebuggingPanel}
fluidRow(
shinyDebuggingPanel::withDebuggingPanel()
)
```


```{r}
includeScript('www/KeyHandler.js')
includeScript('navigateToId.js')   ### ESCAPE key to return.  
###The Chrome problem is NOT:
# the order of these includeScripts.
# due to navigateY. That works in JS console.

includeCSS('TOC.css')    
### includeCSS doesn't work early on.
### It worksin the YAML.
includeCSS('lumpsplit.css')
# includeScript('www/jquery-3.3.1.min.js')
####
```


```{r UI_begins}
require(shinyBS)
```
*Package:T15lumpsplit  `r packageVersion('T15lumpsplit')`  `r packageDate('T15lumpsplit')`     Author: Roger Day, University of Pittsburgh*


# OVERVIEW of this Module  <!-- #title -->


## Contents of this Module  <!-- #title -->

```{r Overview-topic}
conditionalPanelWithCheckbox(
  labelString = 'Overview of the topic', 
  filename = 'Overview.Rmd', 
  initialValue = TRUE)
```

For a refresher on the concepts of bias, variance, and mean squared error in estimation, open this panel.

```{r bias-variance-meansquarederror-MSE.pdf}
conditionalPanelWithCheckboxPDF(labelString='Concepts: bias, variance, and mean squared error in estimation',
     filename="bias-variance-meansquarederror-MSE.pdf",
     cbStringId='cbStringIdBiasVariance')
```

The general scope of the bias-variance axis in many settings is here:

```{r biasVariancePicture}
#### THIS WORKS. In the Preview window, iframe is turned into external to Preview  app. but not from the browser, that's OK. ####

#### bias-variance axis ####
conditionalPanelWithCheckboxPDF(labelString='The bias-variance axis',
     filename="Reproducibility-lump-split-page-1.pdf",
     cbStringId='cbStringIdReproducibility')

```

## Tips on using this Module  <!-- #title -->




```{r Overview-editing}
conditionalPanelWithCheckbox(
  labelString = 'ADVICE on editing data', 
  filename = 'dataEditingAdvice.Rmd', 
  initialValue = FALSE)
```

```{r Overview-saving-retrieving}
conditionalPanelWithCheckbox(
  labelString = 'ADVICE on saving and retrieving your answers questions and comments', 
  filename = 'contentsAdvice.Rmd', 
  initialValue = FALSE)
```


```{r listings}
# ls()
# cat('-=-=-=-=\n')
# ls(pos=1)
# cat('-=-=-=-=\n')
# so it has its own .GlobalEnv, and it's persistent across knits.
# wrapperToGetKeys  ### interesting! that's mine! from DebugTools
```



# "Lumping" versus "splitting" in science. <!-- #title -->


In science, advances often proceed by "splitting": things that seem initially the same are classified as different. The classification may be tentativedistinction, until it proves to be useful. 
Other advances proceed by "lumping":  identifying things that seem different but have a deep connection.

So, we split birds and bats, but we lump bats and whales because they are both mammals. At least in this case, for some purposes, lumping supercedes splitting.
(But if studying modes of  locomotion, the lumping will be different.)

In medicine both splitting and lumping are important.

Cancer was just "cancer" before it was learned through clinical trials that some medicines work well on cancer in one organ but not another--- and vice versa for another medicine.

In recent years, however, specific molecular defects in cancer cells have "lumped" together types of cancer that we would not have dreamed of connecting.
The drug gleevec is a miracle drug for patients with chronic myelogenous leukemia (CML). But not for other leukemias. Deep insight into molecular biology of cancer showed that a radically different kind, gastrointestinal stromal tumor (GIST). 
Better lumping together with better splitting can cure cancer patients.

We will explore a simple lumping and splitting conundrum as a jumping off point for introducing a collection of important statistical ideas that connect to each other.

# TO HERE 327 -- NO jquery-3.3.1.min.js

```{r QA-intro-lump-split}
QandAha(context='QA-intro-lump-split')
```



```{r initialize-jumpLists}
jumpList_DTC = list()
jumpList_BDC = list()
getDTCnumber = function(analysisName) {
  mapAnalysisToDTCnumber[analysisName]
}
getBDCnumber = function(analysisName) {
  mapAnalysisToBDCnumber[analysisName]
}
```

# Hypothetical medical study <!-- #title -->

## Description of the challenge <!-- #title -->

**The Problem**:  A new treatment is given to `r sum(DLdataOriginal)` patients.  
Of them, only `r sum(DLdataOriginal["R", ])` respond.  The outcome we call $Y$.  A responding patient has $Y=R$; non-reponding, $Y=N$.

That is discouraging. 

But there is a subgroup of just `r sum(DLdataOriginal[ , "D"])` in which `r sum(DLdataOriginal["R","D"])` patients respond, yielding a response rate of `r round(100*sum(DLdataOriginal["R","D"])/sum(DLdataOriginal[ , "D"]) )`%. For now we call them "$D$ patients", and the others "$L$ patients". Which subgroup a patient belongs to is a "feature" we call $X_{DL}$, with possible values $D$ or $L$. 

This feature $X_{DL}$ might be a valuable biomarker. Or it might be nonsense.
Possibilities for the actual identity of $X_{DL}$ will be explored later.

**The Challenge**: decide whether to treat the "$D$ patients" -- or more modestly, to decide whether to put resources into studying the treatment further for "$D$ patients". To guide the decision, we need to assess what we know about the **conditional probability**, probability that the patient responds to treatment given that the patient is a $D$ patient. We can see this conditional probability written different ways, some long, some short, using the vertical bar or the dot to say *"conditional on"*:

* Probability of response given a $D$ patient
* $Pr(Y~=~R ~|~ X_{DL} ~=~ D)$
* $Pr(R ~|~ D)$
* $P_{R~.~D}$


# TO HERE 368 -- NO jquery-3.3.1.min.js


## Simple "Lump" and "Split" estimates of $Pr(R ~|~ D)$ <!-- #title -->
```{r analysis-plotLumpSplitPoints}
jumpList_DTC = c(jumpList_DTC, plotLumpSplitPoints='Plot of lump & split points')
source('analysisInitialSetup_DTC.R', local=TRUE); a(name=paste0('section-a_', analysisName) )
#cat("After first analysisInitialSetup_DTC.R, getDTCnumber('plotLumpSplitPoints') =",
#    getDTCnumber('plotLumpSplitPoints'), '\n' )
estimates_Lump_Split = observeEvent( 
  eventExpr = getDLdata('plotLumpSplitPoints'),
  {
    analysisName = 'plotLumpSplitPoints'
  source(analysisReactiveSetup_DTC, local=TRUE)
  #thisData = getDLdata('plotLumpSplitPoints')
  rValues$estimates_Lump_Split = list(
    lump = round(digits=2, sum(thisData['R', ])/
      sum(thisData[ , ])),
    split =  round(digits=2, (thisData['R', 'D'])/
      sum(thisData[ , 'D'])),
    confInterval_Split = binom.test(x = thisData['R', 'D'],
                                    n = sum(thisData[ , 'D']) )$conf.int,
    confInterval_Lump = binom.test(x = sum(thisData['R', ]),
                                   n = sum(thisData[ , ]) )$conf.int
  )
})
 
output$confInterval_Lump = renderText(
  {
  analysisName = 'plotLumpSplitPoints'
  source(analysisReactiveSetup_DTC, local=TRUE)
    try(silent = TRUE,
        paste("Dr. Lump:  ", rValues$estimates_Lump_Split[['lump']],
              "  (", 
          signif(digits=2,
              rValues$estimates_Lump_Split[['confInterval_Lump']][1]),
          "-",
          signif(digits=2,
              rValues$estimates_Lump_Split[['confInterval_Lump']][2]),
          ")"
    ) )
  })
output$confInterval_Split = renderText(
  {
      analysisName = 'plotLumpSplitPoints'
      source(analysisReactiveSetup_DTC, local=TRUE)
      try(silent = TRUE,
          paste("Dr. Split: ", rValues$estimates_Lump_Split[['split']],
              "  (", 
                signif(digits=2,
                       rValues$estimates_Lump_Split[['confInterval_Split']][1]),
                "-",
                signif(digits=2,
                       rValues$estimates_Lump_Split[['confInterval_Split']][2]),
          ")"
    ) )
  }) 

output$plotLumpSplitPoints = renderPlot(
  height = 300, 
  {
    analysisName = 'plotLumpSplitPoints'
    source(analysisReactiveSetup_DTC, local=TRUE)
    cat('plotLumpSplitPoints: renderPlot', paste(thisData), '\n')
    cat('plotLumpSplitPoints: renderPlot', paste(getDLdata(analysisName)), '\n')
    bivariateNormResults = #rValues$bivariateNormResults <<-
      calculatePlightPdarkPosterior(
        DLdata=thisData,
        tau=ifelse(is.null(input$tau), 1, input$tau),
        phi=ifelse(is.null(input$phi), 1, input$phi),
        mu0=ifelse(is.null(input$mu0), 0.5, input$mu0),
        fudgeFactor=ifelse(is.null(input$fudgeFactor), 1e-6, input$fudgeFactor)
      ) 
    cat('plotLumpSplitPoints: ', names(bivariateNormResults), '\n')

    plotPlightPdarkPosterior(
      DLdata=thisData,
      bivariateNormResults=bivariateNormResults,
      showPrior = FALSE, 
      showPosterior=FALSE, showLikelihood=FALSE,
      showS = TRUE,
      showL = TRUE,
      showW = FALSE)
  })
```

Two colleagues on the study hold some strong but opposite opinions.

**Dr. Split** speaks:  “I believe that $X_{DL}$  is an important *biomarker*-- so imporant that the $D$ people and the $L$ people are fundamentally different. Since the question is what to do with $D$ people, I will only look at their data.”  

**Dr. Lump**   disagrees!   “No no, I am sure that this so-called *'biomarker'* $X_{DL}$  is irrelevant! I will ignore it, and look at the data for *all* the patients. My estimate will be more accurate.” 

Examine the data table below. 

(Throughout this module, you can change the numbers in the table and observe the results. Be sure to press "reset" when done.)

*Dr. Split* will *"split"* the sample of patients into the two groups, estimating the response rate `Pr(R | D)` by utilizing the D patients only, is 
`r reactive(round(100*rValues$estimates_Lump_Split$split))`*%*. 

*Dr. Lump* will  *"lump"* together all the patients, estimating that the response rate `Pr(R | D)` for $D$ patients is the same as `Pr(R)`, utilizing the whole set of patients, which is `r reactive(round(100*rValues$estimates_Lump_Split$lump))`*%*. 
  
In the following plot, we see the estimate and the confidence interval for `Pr(R | D)`, depending on whether the *"Lump"* or *"Split"* approach is taken. The confidence interval for *"Split"* is much wider.


# TO HERE 472 -- NO jquery-3.3.1.min.js



```{r plotLumpSplit-UI}
fluidRow(
  column(4, 
         br(),
         div(style='text-align:center;font-weight: bold', 
             "Two estimates for Pr(R | D),",
             br(),
             "with confidence intervals :"),
         tagAppendAttributes(style=paste0("color:", splitColor,
                                          "; text-align:center"),
                             textOutput('confInterval_Split')),
         tagAppendAttributes(style=paste0("color:", lumpColor,
                                          "; text-align:center"),
                             textOutput('confInterval_Lump')),
         plotOutput('plotLumpSplitPoints')
  ),
  column(8,
         dataTableComponent(showhide='show', analysisName=analysisName))
)
```

Lumping gives an answer with low variance ($N$=100 initially; the sample size is fairly high) but high bias-- because the answer reflects far more $L$ patients than $D$ patients. So we get a precise answer to what might be the wrong question. The confidence interval follows the diagonal of the plot on the left above, where $Pr(R~|~D)=Pr(R~|~L)$.

Splitting gives an answer with high variance ($N$=6 initially) but low bias-- because it is including only patients who ARE in the $D$ group:  it is directly asking the right question, but with little precision. The very wide horizontal confidence interval reflects this high variance.

### <a name='section-ClassicalTest'> Classical estimation bifurcating on a hypothesis test</a> <!-- #title -->


```{r panelOfBifurcation}

output$fisherResult = renderUI({
  analysisName = 'plotLumpSplitPoints'
  source(analysisReactiveSetup_DTC, local=TRUE)

  Pvalue = fisher.test(thisData)$p.value
  print(thisData)
  print(DLdataOriginal)
  print(identical(DLdataOriginal, thisData))
  if(identical(DLdataOriginal, thisData))
     "" ### output nothing
     else
       span(
    'Since  you have changed the data in the table above, the P value is now P = ' ,
    strong(em(signif(digits=3, Pvalue))), ".")
})
output$fisherResultOriginal = renderText({
  Pvalue = fisher.test(DLdataOriginal)$p.value
  paste0(
    '   P = ', signif(digits=3, Pvalue), '\n')
})

```

A time-honored but fading technique is to use a classical hypothesis test, to guide which of two analyses "Lump" or "Split", to present.
Here the hypothesis to be tested is " $X_{DL}$ is independent of Y";  the response rate of D patients does not differ from L patients.  Then, depending on the hypothesis test result, Dr.Lump's answer or Dr. Split's answer is selected. 

One test frequently performed to test independence in a 2x2 table like this one is the *Fisher exact test*. On the original data, the P value is 
**`r textOutput('fisherResultOriginal', inline=TRUE)`.** 

 `r uiOutput('fisherResult')`
`r br()`


If the P-value is "significant" (which "by tradition" means "less than 5%"), the "null hypothesis" (that "Dr. Lump" is correct) is rejected. With the original data, the traditional consequence is that we use Dr. Split's estimate:  60%...  and WE COMPLETELY IGNORE THE DATA FOR THE "L" PATIENTS!

```{r QA-fisher}
QandAha('QA-fisher')
```

## Parametrizations of the probabilities: notation <!-- #title -->

To proceed to deeper approaches, we need some notation.

<!-- #### parametrize.Rmd #### -->
```{r child = 'Rmd-text-snippets/parametrize.Rmd'}
```

```{r Why focus on this parameter}
TextQuestion('Why focus on this parameter?')
```


```{r QA-parametrization}
QandAha(context='QA-parametrization')
```

## Approaches to estimating Pr(R | D) <!-- #title -->

* [Classical test: is $X_{DL}$ independent of the outcome Y?](#ClassicalTest)
* [Bayes mixture:  "Dr. Who"](#Bayes_mixture)
* [Bayes joint prior, logit scale](#Bayes_joint_prior).  
* Approaches used in the famous ECMO data set. (ToDo)


### <a name='section-Bayes_mixture'> Approach: Dr. Who's Bayes mixture of "Lump" and "Split"</a> <!-- #title -->

Dr. Who doesn't know *who* to believe.


```{r child = 'DrWho.Rmd'}
```

```{r QA-DrWho}
QandAha('QA-DrWho')
```

###  Approach: Optimizing the mixture with cross-validation <!-- #title -->

Cross-validation is a technique in which the performance of a model can be checked as if on an independent dataset, without actually having a separate dataset. *$K$-fold cross-validation* divides the dataset into K disjoint subsets of observations, and loops over these sets. Each subset is, in turn, used as a `test set`. We remove it from the full dataset, re-build the model on the reduced dataset (the `training set`), and evaluate predictions of that model on the removed observations in the test set. Performance metrics are usually averaged across the $K$ repetitions. When $K=N$, the size of the full dataset, the procedure is called `leave-one-out cross-validation`, since each test set is of size one.

For this example dataset, we perform leave-one-out cross-validation as follows:

```{r Lump-Split-crossvalidation.R}
jumpList_DTC = c(jumpList_DTC, crossvalidationPlot='Cross-validation plot')
source('analysisInitialSetup_DTC.R', local=TRUE); a(name=paste0('section-a_', analysisName) )

source('Lump-Split-crossvalidation.R', local=TRUE)  # , echo = FALSE)

fluidRow(
  column(4, 
         plotOutput('crossvalidationPlot')),
  column(8,
         br(),
         dataTableComponent(analysisName='crossvalidationPlot'))
)
```

```{r TQ and QA for crossvalidation}
TextQuestion("What advantage does the cross-validation approach here have, relative to the approaches of Drs. Split, Lump and Who?")
QandAha('QA-for crossvalidation')
```

### <a name='section-Bayes_joint_prior'> Approach: Bayes joint prior, logit scale.</a> <!-- #title -->

Another Bayesian approach is to set a Bayesian prior on the joint distribution of the two conditional probabilities Pr(R | D) and Pr(R | L). This allows data on the two probabilities to be shared between them, smoothly instead of a crude mixture of the two extreme views of   "Dr. Lump" and "Dr. Split".
```{r QA-smooth-joint-prior}
QandAha('QA-smooth-joint-prior')
```
Once we have this joint prior distribution, we compute the joint posterior.

To set the prior on the joint distribution of Pr(R | D) and Pr(R | L), we first convert them to logits ("logit" means "log(odds)").
```{r Details about the logit}
conditionalPanelWithCheckbox('Details about the logit', filename = 'www/logit.Rmd')
```


Then we set a bivariate normal distribution on the logits.

We also convert the observed proportions to logits. We estimate the variances using the delta method. Details are here:
```{r}
conditionalPanelWithCheckboxPDF(
  labelString="The delta method", 
  filename='www/the-delta-method.pdf',
  cbStringId='delta_method')
```

We can apply the delta method to the Poisson distribution, useful for count data like the data in our table. The variance of the logarithm of a count is roughly the reciprocal of the count.

(This doesn't work well if the count is near zero, and not at all at zero.
For this reason a "continuity fudge" is applied, if necessary.)

```{r}

output$postmean.p = renderText(
  try(silent = TRUE, ## Looks ok despite initial error message
      paste0(
        signif(digits=2, rValues$bivariateNormResults_bivPlot$postmean.p[1]),
        ' (95% confidence interval: ', 
        signif(digits=2, rValues$bivariateNormResults_bivPlot$confints.p[1 , 1]),
        '-',
        signif(digits=2, rValues$bivariateNormResults_bivPlot$confints.p[2 , 1]),
        ')'
      )
  )
)
```

Finally, Pr(R | D) marginalized over Pr(R | L) has posterior mean =  `r  (textOutput('postmean.p', inline=TRUE))` .

```{r linkoutbivariate}
conditionalPanelWithCheckboxPDF(
  labelString='Derivation of the bivariate normal posterior distribution', 
  filename='./lump,split-bivariate-normal-derivation.pdf', 
  cbStringId = 'bivnormPDF')

# observeEvent(eventExpr = {input$bivnorm},
#              { 
#                  linkout(
#                    './lump,split-bivariate-normal-derivation.pdf') 
#              }
# )
```


```{r QA-joint-posterior}
TextQuestion("Your reactions to the derivation?")
QandAha('QA-joint-posterior')
```


```{r BayesLogitPlot}
jumpList_DTC = c(jumpList_DTC, bivariateContourPlot='Bivariate contour plot')
source('analysisInitialSetup_DTC.R', local=TRUE); a(name=paste0('section-a_', analysisName) )

source('bivariateContourPlotReactive.R', local=TRUE)

div(
  fluidRow(
    column(4, 
           tagAppendAttributes(
             div(
               uiOutput(outputId="title_1_ID"),
               uiOutput(outputId="title_2_ID"),
               uiOutput(outputId="title_3_ID")
             ),
             style=
               'text-align:center; font-size:small;
                    font-weight:bold'),
           plotOutput(outputId = 'thePlot', height=260)
    ),
    column(8,
           br(),
           dataTableComponent(analysisName='bivariateContourPlot')
    )
  ),
  fluidRow(
    column(4, 
           ContoursPanelLegend
    ),
    column(8,
           panelOfInputs
    )
  )
)
```

# TO HERE 712 -- NO jquery-3.3.1.min.js



### Summary table of estimates  <!-- #title -->

Assembling the results of the approaches to estimating Pr(R | D):

```{r tableOfEstimates}
jumpList_DTC = c(jumpList_DTC, estimateTable='Table of estimates')
source('analysisInitialSetup_DTC.R', local=TRUE); a(name=paste0('section-a_', analysisName) )

output$estimateTable = 
  renderTable(rownames = TRUE, {
    analysisName = 'estimateTable'
    
    source(analysisReactiveSetup_DTC, local=TRUE);
      ###  Use the last-data for everything.
    
    isolate( {
      cat(paste(thisData), '\n')
      cat(paste(DLdataLastUsed), '\n')
      cat('Copying thisData to other analyses:\n')
      for(aN in 1:analysisNumber) {
        cat('  =>',  names(mapAnalysisToDTCnumber) [aN], ':  ')
        setDLdata(value=thisData, myChoice=TRUE,
                  names(mapAnalysisToDTCnumber) [aN] )
        cat(paste(getDLdata(myChoice=TRUE, 
                            names(mapAnalysisToDTCnumber) [aN]) 
        ), '\n' )
      }
    })
    nRD=thisData['R','D']
    nND=thisData['N','D']
    nR=sum(thisData['R', ])
    nN=sum(thisData['N', ])
    ## Raw results
    LumpRawMean=(nR)/(nN)
    SplitRawMean=(nRD)/(nRD+nND)
    
    ### Dr. Who's results.
    ###  Only for the flat priors. We don't allow Split and Lump priors to change.
    posterior.mean.s = (1+nRD)/(2+nRD+nND)
    posterior.mean.l = (2+nR)/(4+nN)
    pProbSplit = posteriorProb(theData=thisData)
      ### Is this responsive to user's Who prior?
    pProbLump = (1-pProbSplit)
    posterior.mean.w = 
      pProbSplit * posterior.mean.s +
      pProbLump * posterior.mean.l
    
    ## CV results
    setDLdata(value=thisData, analysisName='crossvalidationPlot')
    CVanalysisResult = doCVanalysis(thisData)
    CVoptimalEstimate = CVanalysisResult$CVoptimalEstimate
    CVoptimalWeight = CVanalysisResult$optimalWeight

    ### Bivariate logit method results.
    #setDLdata(value=thisData, analysisName='bivariateContourPlot')
    
    bivariateNormResults = isolate({
      calculatePlightPdarkPosterior(
        thisData, 
        tau=input$tauInput,
        phi=input$phiInput,
        mu0=input$mu0Input
      )
    })
    rValues$bivariateNormResults_bivPlot = bivariateNormResults
    cat("table of estimates: names(bivariateNormResults) ",
        paste(names(bivariateNormResults)), '\n')
    implicitWeight1_ForLogit = 
      (bivariateNormResults$postmean.p[1] - LumpRawMean)/
      (SplitRawMean - LumpRawMean)
    implicitWeights_ForLogit = 
      c(implicitWeight1_ForLogit, 1-implicitWeight1_ForLogit)
    
    niceColHeader = c(
      'Probability estimate Pr(R | D)', 'Weight (Lump)', 'Weight (Split)')
    
    result = data.frame(
      means = c(
        LumpRawMean,
        SplitRawMean,
        posterior.mean.w,
        CVoptimalEstimate,
        bivariateNormResults$postmean.p[1]
      ),
      weightsForLump = c(
        1,
        0,
        pProbLump,
        1-CVoptimalWeight,
        implicitWeights_ForLogit[2]
      ),
      weightsForSplit = c(
        0,
        1,
        pProbSplit,
        CVoptimalWeight,
        implicitWeights_ForLogit[1]
      )
    )
    result = as.data.frame(lapply(result, round, digits=2) )
    rownames(result) = c(
      'Raw proportion (Lump)',
      'Raw proportion (Split)',
      "Dr.Who mixture posterior mean", 
      "Cross-validation optimal",
      'Bivariate inverse logit of logit posterior mean')
    return( (result) )
  })


div(style='fontsize:large;', 
    h4("Posterior probabilities and weights"),
    tableOutput('estimateTable'),
    dataTableComponent(analysisName = 'estimateTable')
)
```

As you change Dr. Who's prior probability or the data values, observe the changes in this summary table. Raw (Lump) & Raw (Split)	are the table proportions taken at face value  by Dr. Lump and Dr. Split. Bayes Mean (Lump)	and Bayes Mean (Split) and Bayes Mean (Who) are Bayesian posterior means that we calculated above. Bayes Pr(Lump) and	Bayes Pr(Split) are Dr. Who's posterior probabilities that Dr. Lump or Dr. Split (respectively) is correct.

```{r QA-Who-prior-probability}
QandAha('QA-Who-prior-probability')
```


# The role of external information, explored by introducing different identities for "D" and "L". <!-- #title -->

So, when should we lump? When should we split? Should we aim to compromise, and if so, how should we strike the balance?  

In this section we explore different scenarios of external information, to see how they affect the estimation of ${Pr(R|D}$.

##	Strong prior belief that feature $X_{DL}$ is *un*important <!-- #title -->

What if D and L represent dark hair color and light hair color? "Dr. Who" may be highly skeptical that this $X_{DL}$ really affects response, and express this by choosing a prior odds that favors Dr. Lump  over Dr. Split. 
##	Strong prior belief that the feature $X_{DL}$ is important <!-- #title -->

What if D and L represent two alleles of a gene believed to affect this drug's pharmacodynamics? Or perhaps, $X_{DL} = D = dark hair$, $X_{DL} = L = light hair$, but hair color is strongly tied to ethnicity in this sample … which is strongly tied to a key genetic variant."Dr. Who" can express choosing a prior odds that favors  Dr. Split over Dr. Lump. 

You can experiment with the effects of prior belief by choosing such a value here: 

```{r WhoPriorProbNumericInput2}
WhoPriorProbNumericInput()
```

and reviewing revised results at these previous locations:
  
```{r QA-Strong-prior-belief-X_DL-important}
inclRmd('jumpBack_DTC.Rmd')
QandAha('QA-Strong-prior-belief-about-X_DL')
```

Which of the previous analyses allow you to interject prior belief on the Lump-versus-Split axis?

##	Multiple features, with some prior belief <!-- #title -->

Suppose that we have features for a hundred genes which were *previously* identified as known to affect the pharmacodynamics of the drug being tested. Our feature $X_{DL}$ happens to be one of them; otherwise unselected.

```{r modify the "classical" approach}
TextQuestion(' How would you modify the "classical" approach that chooses Lump or Split based on a hypothesis test?')
```


```{r Bayesian account for this knowledge }
TextQuestion(' How would you account for this knowledge in one of the two Bayesian approaches?')
```

##	Best feature of 100, with some prior belief <!-- #title -->

As before, suppose that we have features for a hundred genes which were *previously* identified as known to affect the pharmacodynamics of the drug being tested. But this time, our feature $X_{DL}$ is selected to be the best of the one hundred features.

This situation entails dealing with *"multiple comparisons"*. Pure chance can make a feature seem strongly associated with the response outcome when in reality it is not associated. When the best-appearing relationship is selected out of a large number, we can be fooled into thinking the strength of evidence is stronger than it really is.

*"The more questions you ask, the more wrong answers you are likely to get."*
```{r Intro to multiple comparisons}
conditionalPanelWithCheckboxPDF(
  labelString="Intro to multiple comparisons", 
  filename='Multiple comparisons- introduction.pdf',
  cbStringId='multiple_comparisons')
```


One way to handle multiple comparisons is through a classical frequentist multiple testing adjustment. This evaluates the chance of getting a P-value as low as (or lower than) the minimum of the P-values for the features. The best-known of these methods are the Bonferroni, Sidak, and permutation /randomization tests. Here are some details on these approaches: 
```{r Multiple comparisons- classic methods}
conditionalPanelWithCheckboxPDF(
  labelString="Multiple comparisons- classic methods", 
  filename='Multiple comparisons- classic methods.pdf',
  cbStringId='Multiple_comparisons_classic')
```

and here are some examples that highlight some strange and unsettling aspects of the classical methods:

```{r Multiple comparisons- disturbing examples}
conditionalPanelWithCheckboxPDF(
  labelString="Multiple comparisons- disturbing examples", 
  filename='Multiple comparisons- disturbing examples.pdf',
  cbStringId='Multiple_comparisons_disturbing')
```

Let's put those worries aside for now.

What if we add lots of features to the feature ($X_{DL}$) of our little data set? How will the methods we have used behave when applied in this "big" data set.
Let's refer to $X_{DL}$ as $X_1$, and let's generate 99 extra features $X_2 ... X_{100}$ (each with only two possible values, 0 or 1).
<!-- You can use either the original data or the data as you may have modified it. -->


```{r makeDLdataDFYesNoYes  }
#  OK, mission accomplished, example found. We need never run this again.
makeDLdataDFYesNoYes = function(DLdata=DLdataOriginal, qStar=1, upperLimit=95){
  isYesNoYes = FALSE
  seedInt = 40
  while( (! isYesNoYes) & (seedInt < upperLimit)) {
    #savedSeed = .Random.seed
    seedInt = seedInt + 1
    checkYesNoYes(seedInt)
  }
  print(BHtable[ , 1:4])
  saveRDS(seedInt, 'seedInt.rda')
  return(seedInt)
}
checkYesNoYes = function(seed, qStar=1) {
      #DLdataDFResult = makeDLdataDF(DLdata = DLdata)
    allChisqPs = generateAllPvalues(
      makeBigDataWithFeatures(seed=seed) )$allChisqPs
    BHtable = makeBHtable( allChisqPs, qStar )
    isYesNoYes = identical(
      as.vector(BHtable['P<cutoff', 1:3]),
                             c('yes', 'no', 'yes') )
    cat(0+isYesNoYes, seedInt, '\n')
    return(BHtable)
}
```

```{r makeDLdataDF  }
## Make DLdata into a data frame.
makeDLdataDF = function(DLdata=DLdataOriginal) {
  #print(DLdata)
  DLdataDF = 
    data.frame(
      outcome = rep(c("R","N", "R","N"),
                    times=c(DLdata)),
      X_DL = rep(c("D","D", "L","L"),
                 times=c(DLdata))
    )
  return(DLdataDF)
}
## Combine DL feature with extra features. 
makeBigDataWithFeatures = function(DLdata = DLdataOriginal,
                                  ## Make extra features.
                                  NextraFeatures = 99, beta1=10, beta2=10,
                                  Omega = 0,
                                  seed){
  DLdataDF = makeDLdataDF(DLdata)
  if(! missing(seed))
    set.seed(seed)
  extraFeaturesProbs_R <<- rbeta(NextraFeatures, beta1, beta2)
  extraFeaturesOdds_R <<-
    extraFeaturesProbs_R/(1-extraFeaturesProbs_R)
  
  if( abs(Omega) < 1e-9) {
    oddsratios = 1
    extraFeaturesProbs_N <<- extraFeaturesProbs_R
    extraFeaturesOdds_N <<- extraFeaturesOdds_R
    extraFeatures = as.data.frame(
      lapply(extraFeaturesProbs_R,
             rbinom, n=sum(DLdata),size=1)
    )
  } else {
    oddsratios <<- exp(rnorm(NextraFeatures, 0, sqrt(Omega)))
    extraFeaturesOdds_N <<- extraFeaturesOdds_R * oddsratios
    extraFeaturesProbs_N <<-
      extraFeaturesOdds_N/(1+extraFeaturesOdds_N)
    #plot(extraFeaturesProbs_N, extraFeaturesProbs_R)
    extraFeatures_N = sapply(extraFeaturesProbs_N,
                             rbinom, n=sum(DLdataDF$outcome=='N'),size=1)
    extraFeatures_R = sapply(extraFeaturesProbs_R,
                             rbinom, n=sum(DLdataDF$outcome=='R'),size=1)
    extraFeatures =
      matrix(NA, ncol=NextraFeatures, nrow=nrow(DLdataDF))
    extraFeatures[DLdataDF$outcome=='N', ] = extraFeatures_N
    extraFeatures[DLdataDF$outcome=='R', ] = extraFeatures_R
  }
  colnames(extraFeatures) = 
    paste0('X', 1+(1:NextraFeatures))
## Combine DL feature with extra features. 
  DLdataWithFeatures <<- cbind(DLdataDF, extraFeatures ) 
  return(DLdataWithFeatures)
}

#debugonce(makeBigDataWithFeatures)

## generateAllPvalues.
generateAllPvalues = function(
  BigData = makeBigDataWithFeatures()) {
  allFisherPs <<- 
    sapply(BigData[-1], function(feature) 
      fisher.test(feature,
                  BigData$outcome)$p.value
    )
  options.saved = options(warn=-1)
  allChisqPs <<- 
    sapply(BigData[-1], ## removing the outcome column 
           function(feature) {
             result = chisq.test(feature,
                        BigData$outcome)
             # if(max(result$expected) < 5)
             #   result = chisq.test(feature,
             #            DLdataWithFeatures$outcome, simulate.p.value = TRUE)
             result$p.value
           }
    ) 
  options(options.saved)
  return(invisible(
    list(allFisherPs=allFisherPs,
         allChisqPs=allChisqPs))
  ) 
}

#debugonce(generateAllPvalues)


makeSureOmegaIsGood = function(Omega) {
  Omega = as.numeric(Omega)
  if( ! (length(Omega)==1) )
    Omega = 0
  if(is.na(Omega)  | is.nan(Omega) )
    Omega = 0
  return(Omega)
}
```

```{r}

BigDataOriginal = makeBigDataWithFeatures(DLdataOriginal, seed=45)
#print(summary(DLdataDFwithFeatures_original))
allPvalues = generateAllPvalues()
allFisherPs_original = allPvalues$allFisherPs
#print(summary(allFisherPs_original))
allChisqPs_original = allPvalues$allChisqPs
#print(summary(allChisqPs_original))
# BHtable_original = DLdataDF_originalResult$BHtable
```

```{r Pvalues all features}
# observer_Pvalues_all_features = observeEvent(rValues$DLdataDFwithFeatures, {
#              Pvalues = generateAllPvalues(
#                BigData = rValues$DLdataDFwithFeatures
#              )
#              allChisqPs <<- allChisqPs = Pvalues$allChisqPs
#              allFisherP <<- rValues$allFisherPs = Pvalues$allFisherPs
# })

# DLdata = rValues$DLdata, qStar=rValues$qStar)
#     rValues$DLdataDF = makeDLdataDFResult$DLdataDF
#     rValues$allFisherPs = makeDLdataDFResult$allFisherPs
#     allChisqPs = makeDLdataDFResult$allChisqPs
#     allFisherPs <<- rValues$allFisherPs
#     allChisqPs <<- allChisqPs
#   })

```

Now that we have a data set with lots of features, let's do Fisher exact tests and chisquare tests to all the features. The red "X" is of course our original data feature, with $X_{DL}$ = D or L.



```{r pvaluesPlots}
jumpList_BDC = c(jumpList_BDC, pvaluesPlots='P-value plots')
source('analysisInitialSetup_BDC.R', local=TRUE); a(name=paste0('section-a_', analysisName) )
```



```{r pvaluesPlots-react}
output$pvaluesPlots = renderPlot({fun.pvaluesPlots()})
fun.pvaluesPlots = function() {
  analysisName = 'pvaluesPlots'
  source(analysisReactiveSetup_BDC, local=TRUE)
  #hist(rValues$allFisherPs)
  #head(sort(allFisherPs))
  #hist(rValues$allChisqPs)
  #head(sort(allChisqPs))
  plot(allChisqPs, allFisherPs, log='xy')
  points(allChisqPs[1], allFisherPs[1],
         pch='X', col='red', cex=2)
} 

#debug(fun.pvaluesPlots)
plotOutput('pvaluesPlots')
```

For each of these features $X_2 ... X_{100}$, the disparity between the two groups $X_j = 0$ and $X_j = 1$ can be represented as the odds ratio:

\[\frac{{odds(Y = R|{X_j} = 0)}}{{odds(Y = R|{X_j} = 1)}} = \frac{{\Pr (Y = R|{X_j} = 0)/\Pr (Y = N|{X_j} = 0)}}{{\Pr (Y = R|{X_j} = 1)/\Pr (Y = N|{X_j} = 1)}}\]

Initially the extra 99 feature are all "null hypotheses". The odds ratios all equal 1, so the odds that $Y=R$ does not change with $X_j$. But we can explore what happens if they do not all equal 1.
You can control Omega, the variance of logs of the odds ratios below. When Omega equals zero, the odds ratios all equal 1. Otherwise, they will spread out, some bigger than 1 and some smaller.


```{r }
bigDataComponent(analysisName=analysisName)
```

Applying the *"Bonferroni correction"* to the Fisher P value for our feature $X_{DL}$ means multiplying the P value `r signif(digits=4, allFisherPs_original[1])` by 100, to get P=`r signif(digits=4, 100* allFisherPs_original[1])`. So the evidence that the `D` people really are different is minimal at best. (To say it carefully, if a 'cutoff' for defining 'statistically significant' happened to be chosen at `r signif(digits=4, 100* allFisherPs_original[1])`, then our result for $X_{DL}$ would be just on the edge of 'significance'. That's not a compelling cutoff!) 

In the classical statistics spirit, we would now go with Dr. Lump instead of Dr. Split. (Choosing instead the chisquare test, the Bonferroni correction gives us P=`r signif(digits=4, 100* allChisqPs_original[1])`).


# TO HERE 1122 -- NO jquery-3.3.1.min.js


