---
title: "TOC_test"
author: "Roger Day"
date: "7/26/2021"
resource_files:
- www/zoomAdvice.Rmd
- www/zoom_triggers.js
- www/readInnerWindow.js
- www/the-delta-method.pdf
- www/BayesEquation.Rmd
runtime: shiny
output:
  html_document:
    number_sections: yes
    toc: yes
    toc_depth: 6
    toc_float:
      collapsed: no
      smooth_scroll: no
---


\[
\newcommand{\mylarge}[1] {\large{#1}\normalsize}
\newcommand{\mynormal}[1] {{\normalsize{#1}}}
\]

# title 1

# title 2

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = FALSE)
options(knitr.duplicate.label = 'allow')
```


```{r shinyjs, context="server", echo=FALSE, include=TRUE}

#shiny::addResourcePath("shinyjs", system.file("srcjs", package = "shinyjs"))
#library(shinyjs)
shinyjs::useShinyjs(html = TRUE)
require(shinyjs)
shinyjs::useShinyjs(rmd=TRUE, html=TRUE, debug=TRUE)
#<script src="shinyjs/inject.js"></script>

saveEntriesJScode =  
  "shinyjs.saveEntriesJS = function() {document.getElementById('downloadAllUserEntries').click();};"
helloJScode = 
  "shinyjs.helloJS = function() {alert('HELLO');};"
shinyjs::extendShinyjs(text = saveEntriesJScode, functions=c('saveEntriesJS'))
```



```{r include = FALSE}
#  options(error=utils::recover)  # I wish this helped!

user = system('echo $USER', intern = TRUE) # for saving&retrieving files.

require(T15lumpsplit)
require(shiny)
require(xtable)

require(devtools)
if(!require(T15lumpsplit))
  devtools::install_github('professorbeautiful/T15lumpsplit')
if(!require(shinyDebuggingPanel))
  devtools::install_github('professorbeautiful/shinyDebuggingPanel')
require(magrittr);require(shinyBS)
require(knitr);require(rmarkdown);

if(interactive()) {  ### Running out of RStudio locally
  analysisReactiveSetup_DTC = paste0('inst/T15lumpsplit/', 'analysisReactiveSetup_DTC.R')
  analysisReactiveSetup_BDC = paste0('inst/T15lumpsplit/', 'analysisReactiveSetup_BDC.R')
} else  { ### Running at shinyapps.io
  analysisReactiveSetup_DTC =  'analysisReactiveSetup_DTC.R'
  analysisReactiveSetup_BDC =  'analysisReactiveSetup_BDC.R'
}
```

```{r setup numbering, warning=FALSE}
if(exists(x = 'envNumberSequences')) {
  rm(list=ls(envir = envNumberSequences), envir = envNumberSequences)
} else
  envNumberSequences = new.env()
### Note that the .GlobalEnv seen here is different from your R session... and it persists across Rmd runs!

source('initializations.R', local=TRUE)
```
# title 3


```{r colors}
adviceForeground = 'darkred'
adviceBackground = 'snow'
lumpColor = 'red'
splitColor = 'blue'

## https://www.w3schools.com/cssref/css_colors.asp
```

```{r onStop}
#onStop(function() {
   #cat("Doing application cleanup\n")
#})
# Wish it worked.
```

```{r}
#alert('GETTINGSTARTED')  // This works!
includeScript('www/onbeforeunload.js')   ### Not doing anything now?
```




```{r initial values, echo=FALSE}

logit <<- function(p) log(p/(1-p))
antilogit <<- function(x) 1 - 1/(1+exp(x))
DLdataOriginal = matrix(c(3,3,4,90),nrow=2)
dimnames(DLdataOriginal) = list( outcome=c("R","N"), feature=c("D","L"))
ColorForPrior="orange";     
ColorForPosterior="blue";     
ColorForLikelihood="black"
TOCnum =  0
```

```{r servercode}
DLdata = list()
rValues = reactiveValues(tau = 1,  phi = 0.001, mu=0.5,
                         title_1='title 1',
                         #DLdata=list(),
                         #DLdataMyChoice=list(),
                         #DLdataLastUsed = DLdataOriginal,
                         isResetting = FALSE,
                         isLoopingSync = FALSE,
                         WhoPriorProbProb = 1/2,
                         WhoPriorOdds = 1, #,
                         #thisSession = session
                         qStar = 1
)
```


```{r sessioninitialized}
#### This doesn't work in Rmarkdown.
invisible(
  tags$head(tags$script(
    "$(document).on('shiny:sessioninitialized',
    function(event) {
    alert('shiny:sessioninitialized  2');
    Shiny.onInputChange('sessioninitialized2', 2)
});"
  )) 
)
```


```{r shiny:connected}
### this doesn't work either.
invisible(
  tags$head(tags$script(
    "$(document).on('shiny:connected', 
  function(event) {
    alert('shiny:connected  2');
    Shiny.onInputChange('shinyconnected2', 2)
  });"
  )) 
)
```


```{r makeDebuggingPanelOutput}
observeEvent(input$ctrlDpressed, {}) # just to flush the ctrl-D press.

shinyDebuggingPanel::makeDebuggingPanelOutput(
   session, toolsInitialState = TRUE,
   condition='ctrlDpressed === true')


#options(shiny.trace=TRUE)
```

```{r fat arrow, results='asis'}
#cat("\U2B07")
# This shows you can get a fat arrow... but not in a plot.
```

```{r zoomRange}
    #### zoomAdvice ####
zoomRange <<- c(1200, 1400)
```

```{r zoomAdvice}
invisible(inclRmd('www/zoomAdvice.Rmd'))
```


```{r browserAdvice}
invisible(inclRmd('www/browserAdvice.Rmd'))
```

```{r contentsAdvice}
invisible(inclRmd('www/contentsAdvice.Rmd'))
```

```{r in-progress}
h3("NOTE: this is a work in progress!")
```

<a name='section-debugging'> </a>

```{r withDebuggingPanel}
fluidRow(
shinyDebuggingPanel::withDebuggingPanel()
)
```


```{r}
includeScript('www/KeyHandler.js')
includeScript('navigateToId.js')   ### ESCAPE key to return.  
###The Chrome problem is NOT:
# the order of these includeScripts.
# due to navigateY. That works in JS console.

includeCSS('TOC.css')    
### includeCSS doesn't work early on.
### It worksin the YAML.
includeCSS('lumpsplit.css')
# includeScript('www/jquery-3.3.1.min.js')
####
```


```{r UI_begins}
require(shinyBS)
```
*Package:T15lumpsplit  `r packageVersion('T15lumpsplit')`  `r packageDate('T15lumpsplit')`     Author: Roger Day, University of Pittsburgh*


# OVERVIEW of this Module  <!-- #title -->


## Contents of this Module  <!-- #title -->

```{r Overview-topic}
conditionalPanelWithCheckbox(
  labelString = 'Overview of the topic', 
  filename = 'Overview.Rmd', 
  initialValue = TRUE)
```

For a refresher on the concepts of bias, variance, and mean squared error in estimation, open this panel.

```{r bias-variance-meansquarederror-MSE.pdf}
conditionalPanelWithCheckboxPDF(labelString='Concepts: bias, variance, and mean squared error in estimation',
     filename="bias-variance-meansquarederror-MSE.pdf",
     cbStringId='cbStringIdBiasVariance')
```

The general scope of the bias-variance axis in many settings is here:

```{r biasVariancePicture}
#### THIS WORKS. In the Preview window, iframe is turned into external to Preview  app. but not from the browser, that's OK. ####

#### bias-variance axis ####
conditionalPanelWithCheckboxPDF(labelString='The bias-variance axis',
     filename="Reproducibility-lump-split-page-1.pdf",
     cbStringId='cbStringIdReproducibility')

```

## Tips on using this Module  <!-- #title -->




```{r Overview-editing}
conditionalPanelWithCheckbox(
  labelString = 'ADVICE on editing data', 
  filename = 'dataEditingAdvice.Rmd', 
  initialValue = FALSE)
```

```{r Overview-saving-retrieving}
conditionalPanelWithCheckbox(
  labelString = 'ADVICE on saving and retrieving your answers questions and comments', 
  filename = 'contentsAdvice.Rmd', 
  initialValue = FALSE)
```


```{r listings}
# ls()
# cat('-=-=-=-=\n')
# ls(pos=1)
# cat('-=-=-=-=\n')
# so it has its own .GlobalEnv, and it's persistent across knits.
# wrapperToGetKeys  ### interesting! that's mine! from DebugTools
```



# "Lumping" versus "splitting" in science. <!-- #title -->


In science, advances often proceed by "splitting": things that seem initially the same are classified as different. The classification may be tentativedistinction, until it proves to be useful. 
Other advances proceed by "lumping":  identifying things that seem different but have a deep connection.

So, we split birds and bats, but we lump bats and whales because they are both mammals. At least in this case, for some purposes, lumping supercedes splitting.
(But if studying modes of  locomotion, the lumping will be different.)

In medicine both splitting and lumping are important.

Cancer was just "cancer" before it was learned through clinical trials that some medicines work well on cancer in one organ but not another--- and vice versa for another medicine.

In recent years, however, specific molecular defects in cancer cells have "lumped" together types of cancer that we would not have dreamed of connecting.
The drug gleevec is a miracle drug for patients with chronic myelogenous leukemia (CML). But not for other leukemias. Deep insight into molecular biology of cancer showed that a radically different kind, gastrointestinal stromal tumor (GIST). 
Better lumping together with better splitting can cure cancer patients.

We will explore a simple lumping and splitting conundrum as a jumping off point for introducing a collection of important statistical ideas that connect to each other.

# TO HERE 327 -- NO jquery-3.3.1.min.js

```{r QA-intro-lump-split}
QandAha(context='QA-intro-lump-split')
```



```{r initialize-jumpLists}
jumpList_DTC = list()
jumpList_BDC = list()
getDTCnumber = function(analysisName) {
  mapAnalysisToDTCnumber[analysisName]
}
getBDCnumber = function(analysisName) {
  mapAnalysisToBDCnumber[analysisName]
}
```

# Hypothetical medical study <!-- #title -->

## Description of the challenge <!-- #title -->

**The Problem**:  A new treatment is given to `r sum(DLdataOriginal)` patients.  
Of them, only `r sum(DLdataOriginal["R", ])` respond.  The outcome we call $Y$.  A responding patient has $Y=R$; non-reponding, $Y=N$.

That is discouraging. 

But there is a subgroup of just `r sum(DLdataOriginal[ , "D"])` in which `r sum(DLdataOriginal["R","D"])` patients respond, yielding a response rate of `r round(100*sum(DLdataOriginal["R","D"])/sum(DLdataOriginal[ , "D"]) )`%. For now we call them "$D$ patients", and the others "$L$ patients". Which subgroup a patient belongs to is a "feature" we call $X_{DL}$, with possible values $D$ or $L$. 

This feature $X_{DL}$ might be a valuable biomarker. Or it might be nonsense.
Possibilities for the actual identity of $X_{DL}$ will be explored later.

**The Challenge**: decide whether to treat the "$D$ patients" -- or more modestly, to decide whether to put resources into studying the treatment further for "$D$ patients". To guide the decision, we need to assess what we know about the **conditional probability**, probability that the patient responds to treatment given that the patient is a $D$ patient. We can see this conditional probability written different ways, some long, some short, using the vertical bar or the dot to say *"conditional on"*:

* Probability of response given a $D$ patient
* $Pr(Y~=~R ~|~ X_{DL} ~=~ D)$
* $Pr(R ~|~ D)$
* $P_{R~.~D}$


# TO HERE 368 -- NO jquery-3.3.1.min.js


## Simple "Lump" and "Split" estimates of $Pr(R ~|~ D)$ <!-- #title -->
```{r analysis-plotLumpSplitPoints}
jumpList_DTC = c(jumpList_DTC, plotLumpSplitPoints='Plot of lump & split points')
source('analysisInitialSetup_DTC.R', local=TRUE); a(name=paste0('section-a_', analysisName) )
#cat("After first analysisInitialSetup_DTC.R, getDTCnumber('plotLumpSplitPoints') =",
#    getDTCnumber('plotLumpSplitPoints'), '\n' )
estimates_Lump_Split = observeEvent( 
  eventExpr = getDLdata('plotLumpSplitPoints'),
  {
    analysisName = 'plotLumpSplitPoints'
  source(analysisReactiveSetup_DTC, local=TRUE)
  #thisData = getDLdata('plotLumpSplitPoints')
  rValues$estimates_Lump_Split = list(
    lump = round(digits=2, sum(thisData['R', ])/
      sum(thisData[ , ])),
    split =  round(digits=2, (thisData['R', 'D'])/
      sum(thisData[ , 'D'])),
    confInterval_Split = binom.test(x = thisData['R', 'D'],
                                    n = sum(thisData[ , 'D']) )$conf.int,
    confInterval_Lump = binom.test(x = sum(thisData['R', ]),
                                   n = sum(thisData[ , ]) )$conf.int
  )
})
 
output$confInterval_Lump = renderText(
  {
  analysisName = 'plotLumpSplitPoints'
  source(analysisReactiveSetup_DTC, local=TRUE)
    try(silent = TRUE,
        paste("Dr. Lump:  ", rValues$estimates_Lump_Split[['lump']],
              "  (", 
          signif(digits=2,
              rValues$estimates_Lump_Split[['confInterval_Lump']][1]),
          "-",
          signif(digits=2,
              rValues$estimates_Lump_Split[['confInterval_Lump']][2]),
          ")"
    ) )
  })
output$confInterval_Split = renderText(
  {
      analysisName = 'plotLumpSplitPoints'
      source(analysisReactiveSetup_DTC, local=TRUE)
      try(silent = TRUE,
          paste("Dr. Split: ", rValues$estimates_Lump_Split[['split']],
              "  (", 
                signif(digits=2,
                       rValues$estimates_Lump_Split[['confInterval_Split']][1]),
                "-",
                signif(digits=2,
                       rValues$estimates_Lump_Split[['confInterval_Split']][2]),
          ")"
    ) )
  }) 

output$plotLumpSplitPoints = renderPlot(
  height = 300, 
  {
    analysisName = 'plotLumpSplitPoints'
    source(analysisReactiveSetup_DTC, local=TRUE)
    cat('plotLumpSplitPoints: renderPlot', paste(thisData), '\n')
    cat('plotLumpSplitPoints: renderPlot', paste(getDLdata(analysisName)), '\n')
    bivariateNormResults = #rValues$bivariateNormResults <<-
      calculatePlightPdarkPosterior(
        DLdata=thisData,
        tau=ifelse(is.null(input$tau), 1, input$tau),
        phi=ifelse(is.null(input$phi), 1, input$phi),
        mu0=ifelse(is.null(input$mu0), 0.5, input$mu0),
        fudgeFactor=ifelse(is.null(input$fudgeFactor), 1e-6, input$fudgeFactor)
      ) 
    cat('plotLumpSplitPoints: ', names(bivariateNormResults), '\n')

    plotPlightPdarkPosterior(
      DLdata=thisData,
      bivariateNormResults=bivariateNormResults,
      showPrior = FALSE, 
      showPosterior=FALSE, showLikelihood=FALSE,
      showS = TRUE,
      showL = TRUE,
      showW = FALSE)
  })
```

Two colleagues on the study hold some strong but opposite opinions.

**Dr. Split** speaks:  “I believe that $X_{DL}$  is an important *biomarker*-- so imporant that the $D$ people and the $L$ people are fundamentally different. Since the question is what to do with $D$ people, I will only look at their data.”  

**Dr. Lump**   disagrees!   “No no, I am sure that this so-called *'biomarker'* $X_{DL}$  is irrelevant! I will ignore it, and look at the data for *all* the patients. My estimate will be more accurate.” 

Examine the data table below. 

(Throughout this module, you can change the numbers in the table and observe the results. Be sure to press "reset" when done.)

*Dr. Split* will *"split"* the sample of patients into the two groups, estimating the response rate `Pr(R | D)` by utilizing the D patients only, is 
`r reactive(round(100*rValues$estimates_Lump_Split$split))`*%*. 

*Dr. Lump* will  *"lump"* together all the patients, estimating that the response rate `Pr(R | D)` for $D$ patients is the same as `Pr(R)`, utilizing the whole set of patients, which is `r reactive(round(100*rValues$estimates_Lump_Split$lump))`*%*. 
  
In the following plot, we see the estimate and the confidence interval for `Pr(R | D)`, depending on whether the *"Lump"* or *"Split"* approach is taken. The confidence interval for *"Split"* is much wider.


# TO HERE 472 -- NO jquery-3.3.1.min.js



```{r plotLumpSplit-UI}
fluidRow(
  column(4, 
         br(),
         div(style='text-align:center;font-weight: bold', 
             "Two estimates for Pr(R | D),",
             br(),
             "with confidence intervals :"),
         tagAppendAttributes(style=paste0("color:", splitColor,
                                          "; text-align:center"),
                             textOutput('confInterval_Split')),
         tagAppendAttributes(style=paste0("color:", lumpColor,
                                          "; text-align:center"),
                             textOutput('confInterval_Lump')),
         plotOutput('plotLumpSplitPoints')
  ),
  column(8,
         dataTableComponent(showhide='show', analysisName=analysisName))
)
```

Lumping gives an answer with low variance ($N$=100 initially; the sample size is fairly high) but high bias-- because the answer reflects far more $L$ patients than $D$ patients. So we get a precise answer to what might be the wrong question. The confidence interval follows the diagonal of the plot on the left above, where $Pr(R~|~D)=Pr(R~|~L)$.

Splitting gives an answer with high variance ($N$=6 initially) but low bias-- because it is including only patients who ARE in the $D$ group:  it is directly asking the right question, but with little precision. The very wide horizontal confidence interval reflects this high variance.

### <a name='section-ClassicalTest'> Classical estimation bifurcating on a hypothesis test</a> <!-- #title -->


```{r panelOfBifurcation}

output$fisherResult = renderUI({
  analysisName = 'plotLumpSplitPoints'
  source(analysisReactiveSetup_DTC, local=TRUE)

  Pvalue = fisher.test(thisData)$p.value
  print(thisData)
  print(DLdataOriginal)
  print(identical(DLdataOriginal, thisData))
  if(identical(DLdataOriginal, thisData))
     "" ### output nothing
     else
       span(
    'Since  you have changed the data in the table above, the P value is now P = ' ,
    strong(em(signif(digits=3, Pvalue))), ".")
})
output$fisherResultOriginal = renderText({
  Pvalue = fisher.test(DLdataOriginal)$p.value
  paste0(
    '   P = ', signif(digits=3, Pvalue), '\n')
})

```

A time-honored but fading technique is to use a classical hypothesis test, to guide which of two analyses "Lump" or "Split", to present.
Here the hypothesis to be tested is " $X_{DL}$ is independent of Y";  the response rate of D patients does not differ from L patients.  Then, depending on the hypothesis test result, Dr.Lump's answer or Dr. Split's answer is selected. 

One test frequently performed to test independence in a 2x2 table like this one is the *Fisher exact test*. On the original data, the P value is 
**`r textOutput('fisherResultOriginal', inline=TRUE)`.** 

 `r uiOutput('fisherResult')`
`r br()`


If the P-value is "significant" (which "by tradition" means "less than 5%"), the "null hypothesis" (that "Dr. Lump" is correct) is rejected. With the original data, the traditional consequence is that we use Dr. Split's estimate:  60%...  and WE COMPLETELY IGNORE THE DATA FOR THE "L" PATIENTS!

```{r QA-fisher}
QandAha('QA-fisher')
```

## Parametrizations of the probabilities: notation <!-- #title -->

To proceed to deeper approaches, we need some notation.

<!-- #### parametrize.Rmd #### -->
```{r child = 'Rmd-text-snippets/parametrize.Rmd'}
```

```{r Why focus on this parameter}
TextQuestion('Why focus on this parameter?')
```


```{r QA-parametrization}
QandAha(context='QA-parametrization')
```

## Approaches to estimating Pr(R | D) <!-- #title -->

* [Classical test: is $X_{DL}$ independent of the outcome Y?](#ClassicalTest)
* [Bayes mixture:  "Dr. Who"](#Bayes_mixture)
* [Bayes joint prior, logit scale](#Bayes_joint_prior).  
* Approaches used in the famous ECMO data set. (ToDo)


### <a name='section-Bayes_mixture'> Approach: Dr. Who's Bayes mixture of "Lump" and "Split"</a> <!-- #title -->

Dr. Who doesn't know *who* to believe.


```{r child = 'DrWho.Rmd'}
```

```{r QA-DrWho}
QandAha('QA-DrWho')
```

###  Approach: Optimizing the mixture with cross-validation <!-- #title -->

Cross-validation is a technique in which the performance of a model can be checked as if on an independent dataset, without actually having a separate dataset. *$K$-fold cross-validation* divides the dataset into K disjoint subsets of observations, and loops over these sets. Each subset is, in turn, used as a `test set`. We remove it from the full dataset, re-build the model on the reduced dataset (the `training set`), and evaluate predictions of that model on the removed observations in the test set. Performance metrics are usually averaged across the $K$ repetitions. When $K=N$, the size of the full dataset, the procedure is called `leave-one-out cross-validation`, since each test set is of size one.

For this example dataset, we perform leave-one-out cross-validation as follows:

```{r Lump-Split-crossvalidation.R}
jumpList_DTC = c(jumpList_DTC, crossvalidationPlot='Cross-validation plot')
source('analysisInitialSetup_DTC.R', local=TRUE); a(name=paste0('section-a_', analysisName) )

source('Lump-Split-crossvalidation.R', local=TRUE)  # , echo = FALSE)

fluidRow(
  column(4, 
         plotOutput('crossvalidationPlot')),
  column(8,
         br(),
         dataTableComponent(analysisName='crossvalidationPlot'))
)
```

```{r TQ and QA for crossvalidation}
TextQuestion("What advantage does the cross-validation approach here have, relative to the approaches of Drs. Split, Lump and Who?")
QandAha('QA-for crossvalidation')
```

### <a name='section-Bayes_joint_prior'> Approach: Bayes joint prior, logit scale.</a> <!-- #title -->

Another Bayesian approach is to set a Bayesian prior on the joint distribution of the two conditional probabilities Pr(R | D) and Pr(R | L). This allows data on the two probabilities to be shared between them, smoothly instead of a crude mixture of the two extreme views of   "Dr. Lump" and "Dr. Split".
```{r QA-smooth-joint-prior}
QandAha('QA-smooth-joint-prior')
```
Once we have this joint prior distribution, we compute the joint posterior.

To set the prior on the joint distribution of Pr(R | D) and Pr(R | L), we first convert them to logits ("logit" means "log(odds)").
```{r Details about the logit}
conditionalPanelWithCheckbox('Details about the logit', filename = 'www/logit.Rmd')
```


Then we set a bivariate normal distribution on the logits.

We also convert the observed proportions to logits. We estimate the variances using the delta method. Details are here:
```{r}
conditionalPanelWithCheckboxPDF(
  labelString="The delta method", 
  filename='www/the-delta-method.pdf',
  cbStringId='delta_method')
```

We can apply the delta method to the Poisson distribution, useful for count data like the data in our table. The variance of the logarithm of a count is roughly the reciprocal of the count.

(This doesn't work well if the count is near zero, and not at all at zero.
For this reason a "continuity fudge" is applied, if necessary.)

```{r}

output$postmean.p = renderText(
  try(silent = TRUE, ## Looks ok despite initial error message
      paste0(
        signif(digits=2, rValues$bivariateNormResults_bivPlot$postmean.p[1]),
        ' (95% confidence interval: ', 
        signif(digits=2, rValues$bivariateNormResults_bivPlot$confints.p[1 , 1]),
        '-',
        signif(digits=2, rValues$bivariateNormResults_bivPlot$confints.p[2 , 1]),
        ')'
      )
  )
)
```

Finally, Pr(R | D) marginalized over Pr(R | L) has posterior mean =  `r  (textOutput('postmean.p', inline=TRUE))` .

```{r linkoutbivariate}
conditionalPanelWithCheckboxPDF(
  labelString='Derivation of the bivariate normal posterior distribution', 
  filename='./lump,split-bivariate-normal-derivation.pdf', 
  cbStringId = 'bivnormPDF')

# observeEvent(eventExpr = {input$bivnorm},
#              { 
#                  linkout(
#                    './lump,split-bivariate-normal-derivation.pdf') 
#              }
# )
```


```{r QA-joint-posterior}
TextQuestion("Your reactions to the derivation?")
QandAha('QA-joint-posterior')
```


```{r BayesLogitPlot}
jumpList_DTC = c(jumpList_DTC, bivariateContourPlot='Bivariate contour plot')
source('analysisInitialSetup_DTC.R', local=TRUE); a(name=paste0('section-a_', analysisName) )

source('bivariateContourPlotReactive.R', local=TRUE)

div(
  fluidRow(
    column(4, 
           tagAppendAttributes(
             div(
               uiOutput(outputId="title_1_ID"),
               uiOutput(outputId="title_2_ID"),
               uiOutput(outputId="title_3_ID")
             ),
             style=
               'text-align:center; font-size:small;
                    font-weight:bold'),
           plotOutput(outputId = 'thePlot', height=260)
    ),
    column(8,
           br(),
           dataTableComponent(analysisName='bivariateContourPlot')
    )
  ),
  fluidRow(
    column(4, 
           ContoursPanelLegend
    ),
    column(8,
           panelOfInputs
    )
  )
)
```

# TO HERE 712 -- NO jquery-3.3.1.min.js

